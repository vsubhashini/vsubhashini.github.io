<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Subhashini Venugopalan</title>

    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/template.css" rel="stylesheet">
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    
      ga('create', 'UA-59771467-2', 'auto');
      ga('send', 'pageview');
    
    </script>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
          <!-- Brand and toggle get grouped for better mobile display -->
        <div class="container">

          <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
          </div>
    
          <!-- Collect the nav links, forms, and other content for toggling -->
          <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
              <li><a href="./index.html#">Home</a></li>
              <li><a href="./index.html#Research">Research</a></li>
              <li><a href="./index.html#Contact">Contact</a></li>
            </ul
          </div><!-- /.navbar-collapse -->

        </div>
    </nav>


    <div class="container">

    <div class="home-intro">
      <div class="row">
          <h1 align="center">
          Captioning Images with Diverse Objects
          </h1>
      </div>
          <div class="highlight-box">
           <h2>
           <div class="center-pills">
           <ul class="nav nav-pills">
          <li role="presentation"> <a href="#abstract">Abstract</a></li> 
          <li role="presentation"> <a href="#overview">Overview</a></li> 
          <li role="presentation"> <a href="#examples">Examples</a></li>
          <li role="presentation"> <a href="#code">Code</a></li> 
          <li role="presentation"> <a href="#refs">Reference</a></li>
           </ul>
          </div>
          </h2>
          </div>

          <div class="project-page">
          <a name="abstract"></a>
          <h2>Abstract</h2>
          <p class="text-justify">
          Recent captioning models are limited in their ability to scale and
          describe concepts unseen in 
          paired image-text corpora. We propose the Novel Object Captioner
          (<b>NOC</b>), a deep visual semantic captioning model that can
          describe a large number of object categories not present in existing
          image-caption datasets. Our model takes advantage of external sources
          -- labeled images from object recognition datasets, and semantic
          knowledge extracted from unannotated text. 
          We propose minimizing a joint objective which can learn from these
          diverse data sources and leverage distributional semantic embeddings,
          enabling the model to generalize and describe novel objects outside of
          image-caption datasets. 
          We demonstrate that our model exploits semantic information to
          generate captions for hundreds of object categories in the ImageNet
          object recognition dataset that are not observed in MSCOCO
          image-caption training data, as well as many categories that are
          observed very rarely. Both automatic evaluations and human judgements
          show that our model considerably outperforms prior work in being able
          to describe many more categories of objects.
          </p>
          <center>
          <a
          href="https://arxiv.org/abs/1606.07770" class="btn btn-danger" role="button">PDF</a>
          <a
          href="https://drive.google.com/open?id=0Bxz2Bk18GoW9Y3VaSldBUlctNWc"
          class="btn btn-warning"
          role="button">Poster</a>
         <a
          href="https://drive.google.com/open?id=0Bxz2Bk18GoW9TzRrMEZ0VVdKbzA" class="btn btn-success"
          role="button">Slides</a>
         <a
          href="https://youtu.be/OQNoy4pgDr4" class="btn btn-primary"
          role="button">Talk</a>
          </center>
          </div>

    <hr class="soften"></hr>

          <div class="project-page">
          <a name="overview"></a>
          <h2>Overview</h2>
          <center>
          <p class="lead">Our goal is to generate captions for novel objects not
          present in paired image-caption training data but exist in image
          recognition datasets e.g. ImageNet. <a
          href="http://bair.berkeley.edu/blog/2017/08/08/novel-object-captioning/">(More
          in the blog post here.)</a></p>
          <div class="approachimg">
            <img class="center-block" src="./imgs/noc/noc-teaser.svg"
            alt="Captioning ImageNet objects without corresponding caption training data."></img>
          </div>
          </center>
          <center>
          <p class="text-justify">
          We propose a joint training strategy with auxiliary objectives which
          allows our network to learn a captioning model on image-caption pairs
          simultaneously with a deep language model
          and visual recognition system on unannotated text and labeled images.
          Unlike previous work, the auxiliary objectives allow
          the NOC model to learn relevant information
          from multiple data sources simultaneously in an end-to-end
          fashion.
          </p>
          <div class="approachimg">
            <img class="center-block" src="./imgs/noc/noc-model.svg"
            alt="Late and Deep Fusion."></img>
          </div>
          <p class="text-justify">
          Typically, image-captioning models incorporate a visual classifier
          pre-trained on a source domain (e.g. ImageNet dataset) and then tune
          it to the target domain (the image-caption dataset). However,
          important information from the source dataset can be suppressed if
          similar information is not present when fine-tuning, leading the
          network to forget (over-write weights) for objects not present in the
          target domain. Our joint training strategy remedies this.
          Additionally, our language model incorporates dense representation of
          words on both the input and output allowing the model to compose
          sentences about novel objects.
          </p>
          </center>


          </div>

    <hr class="soften"></hr>

          <div class="project-page">
          <a name="examples"></a>
          <h2>Examples</h2>
          <center>
            <p class=lead">
             <a href="https://vsubhashini.github.io/noc_examples.html">Click here
            for lots more examples.</a>
            </p>
          <div class="approachimg">
            <img class="center-block" src="./imgs/noc/ex_noc_context.svg"
            alt="Captioning novel objects in context."></img>
          </div>
          <p class="lead">
          Example captions of ImageNet objects in different contexts.
          </p>
          <div class="approachimg">
            <img class="center-block" src="./imgs/noc/ex_imnet_compare_dcc_noc.svg"
            alt="Comparing captions from DCC and NOC."></img>
          </div>
           <p class="lead">
            Comparing captions generated by our NOC model with prior work (DCC).
            </br>
            <a
            href="https://vsubhashini.github.io/noc_examples.html">Click here
            for lots more examples.
            </a>
          </p>
          </div>
          <!--
          <div class="approachimg">
            <img class="center-block" src="./imgs/lm_fusion/emnlp_fig.svg"
            alt="Movie example."></img>
            <p class="lead">
            An example from one clip in the movie dataset.
            </p>
          </div>
          -->
          </center>
          </div>

    <hr class="soften"></hr>

          <div class="project-page">
          <a name="code"></a>

          <h2>Code and Generated Captions</h2>
          <!--<h1><font color="FF0000">COMING SOON</font></h1>-->
          <h3>Download Captions</h3>
          <p class="lead">
            <a
            href="https://drive.google.com/drive/folders/0Bxz2Bk18GoW9S184eTRsOFBSb0U?usp=sharing">Download
            all captions here.</a>
            </br>
          <dl style="font-size:16px;">
            Captions generated by NOC on MSCOCO images containing the 8 held-out
            objects.
            </br>
            <a
            href="https://drive.google.com/open?id=0Bxz2Bk18GoW9dWVxYVVkUnN1aHM">Beam
            search width 1 (F1: 50.51, Meteor: 20.69)</a>
            </br>
            <a
            href="https://drive.google.com/open?id=0Bxz2Bk18GoW9MXdQbnZYWGxReW8">Sampled
            (F1: 48.79, Meteor: 21.32)</a>
            </br>
            </br>
            Captions generated by NOC on ImageNet test images of objects never
            mentioned in COCO.
            </br>
            <a
            href="https://drive.google.com/open?id=0Bxz2Bk18GoW9ZDJKMnN3aWIyWVU">ImageNet
            testset captions</a>
            </br>
            </br>
            Captions generated by NOC on ImageNet test images of objects
            mentioned <i>rarely</i> in COCO.
            </br>
            <a
            href="https://drive.google.com/open?id=0Bxz2Bk18GoW9Z3JJaUpXUmI4em8">Rare
            objects captions</a>
            </br>
            </br>
            ImageNet images and captions sampled for Human Evaluation (using
            Amazon Mechanical Turk).
            </br>
            <a
            href="https://drive.google.com/open?id=0Bxz2Bk18GoW9MmxURlg4OXZWejQ">Objects
            which both NOC and DCC can describe (Intersection)</a>
            </br>
            <a
            href="https://drive.google.com/open?id=0Bxz2Bk18GoW9NGFQOWozWUxFaHc">Objects
            which either NOC or DCC can describe (Union)</a>
            </br>
            </br>
            </dl>
          </p>
          <p class="lead">
            The code to prepare data and train the model can be found in:
            </br>
            <!--<a
            href="https://github.com/vsubhashini/caffe/tree/recurrent/examples/language_fusion">https://github.com/vsubhashini/caffe/tree/recurrent/examples/language_fusion</a>
            </br>
            -->
            <!--
            </br>
            Model information:<a
            href="https://gist.github.com/vsubhashini/38d087e140854fee4b14"> GitHub_Gist</a>
            -->
            </br>
            Download pre-trained model: 
            <!--<a
            href="https://www.dropbox.com/s/7g9qt4bt1p7q5bw/lm_deepfus_img512_s2vt_glove_72.7kvocab_sgd_lr2e3step7k_iter_16000.caffemodel?dl=1">InDomain_DeepFusion_Model</a>
            (741MB)-->
            </br>
            </br>
            Evaluation Code:
            <a
            href="https://github.com/vsubhashini/caption-eval">https://github.com/vsubhashini/caption-eval</a>

          </p>
          </div>

    <hr class="soften"></hr>

          <div class="project-page">
          <a name="refs"></a>
          <h2>Reference</h2>
          <p class="lead">
          If you find this useful in your work please consider citing:
          <div class="highlight">
          <pre> <code> 
          @inproceedings{venugopalan17cvpr,
          title = {Captioning Images with Diverse Objects},
          author={Venugopalan, Subhashini and Hendricks, Lisa Anne and Rohrbach,
          Marcus and Mooney, Raymond, and Darrell, Trevor and Saenko, Kate},
          booktitle = {Proceedings of the IEEE Conference on Computer Vision and
          Pattern Recognition (CVPR)},
          year = {2017}
          }
 </code> </pre>

          </div>

    </div>
    </div> <!--container-->

    <footer class="footer">
        <div class="container">
            <p class="text-muted text-center" style="padding-top: 10px">
                <a title="Creative Commons Attribution 4.0 International license" target="_blank" href="http://creativecommons.org/licenses/by/4.0/" rel="license">
                  <img alt="License" src="imgs/cca4-88x31.png"></img>
                </a>
            </p>
        </div>
    </footer>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>
    <script src="js/toggle.js"></script>
  </body>
</html>

