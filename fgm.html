<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Subhashini Venugopalan</title>

    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/template.css" rel="stylesheet">
    <script type="text/javascript"><!-- 
    function obfuscate( domain, name ) { document.write('<a href="mai' + 
    'lto:' + name + '@' + domain + '">' + name + '@' + domain + '</' + 'a>'); }
    // --></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-59771467-2', 'auto');
  ga('send', 'pageview');

</script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
          <!-- Brand and toggle get grouped for better mobile display -->
        <div class="container">

          <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
          </div>
    
          <!-- Collect the nav links, forms, and other content for toggling -->
          <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
              <li><a href="./index.html#">Home</a></li>
              <li><a href="./index.html#Research">Research</a></li>
              <li><a href="./index.html#Contact">Contact</a></li>
            </ul
          </div><!-- /.navbar-collapse -->

        </div>
    </nav>


    <div class="container">

    <div class="home-intro">
      <div class="row">
          <h1 align="center">Integrating Language and Vision to Generate Natural
          Language Descriptions of Videos in the Wild</h1>
      </div>
          <div class="highlight-box">
           <h2>
           <div class="center-pills">
           <ul class="nav nav-pills">
          <li role="presentation"> <a href="#abstract">Abstract</a></li> 
          <li role="presentation"> <a href="#overview">Overview</a></li> 
          <li role="presentation"> <a href="#examples">Examples</a></li>
          <li role="presentation"> <a href="#code">Code</a></li> 
          <li role="presentation"> <a href="#refs">Reference</a></li>
           </ul>
          </div>
          </h2>
          </div>

          <div class="project-page">
          <a name="abstract"></a>
          <h2>Abstract</h2>
          <p class="text-justify">
          This paper integrates techniques in natural language processing and
          computer vision to improve recognition and description of entities and
          activities in real-world videos. We propose a strategy for generating
          textual descriptions of videos by using a factor graph to combine
          visual detections with language statistics. We use state-of-the-art
          visual recognition systems to obtain confidences on entities,
          activities, and scenes present in the video. Our factor graph model
          combines these detection confidences with probabilistic knowledge
          mined from text corpora to estimate the most likely subject, verb,
          object, and place. Results on YouTube videos show that our approach
          improves both the joint detection of these latent, diverse sentence
          components and the detection of some individual components when
          compared to using the vision system alone, as well as over a previous
          n-gram language-modeling approach. The joint detection allows us to
          automatically generate more accurate, richer sentential descriptions
          of videos with a wide array of possible content. 
          </p>
          <center>
          <a
          href="http://www.cs.utexas.edu/users/ml/papers/thomason.coling14.pdf" class="btn btn-danger" role="button">PDF</a>
          <a
          href="pdf/FGM-coling14-slides.pdf" class="btn btn-success"
          role="button">Slides</a>
          <a
          href="pdf/fgm_poster.pdf"
          class="btn btn-warning"
          role="button">Poster</a>
          </center>
          </div>

    <hr class="soften"></hr>

          <div class="project-page">
          <a name="overview"></a>
          <h2>Overview</h2>
          <center>
          <div class="approachimg">
            <img class="center-block"
            src="imgs/nsf-fgm.png" alt="FGM overview"></img>
          </div>
          <p class="lead">An overview of our factor graph model that integrates
          confidences from visual classifiers with statistics mined from large
          text corpora to estimate the most likely Subject-Verb-Object-Place
          tuple.</p>
          </center>

          </div>

    <hr class="soften"></hr>

          <div class="project-page">
          <a name="examples"></a>
          <h2>Examples</h2>
          <center>
          <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"
          src="https://www.youtube.com/embed/pShM8CVAYxI"
          allowfullscreen></iframe>
          </div>
          <p class="lead">
          Sample Youtube clips with outputs from the FGM model.
          </p>
          </center>
          </div>

    <hr class="soften"></hr>

          <div class="project-page">
          <a name="code"></a>
          <h2>Code</h2>
          <p class="lead">
          
            </br>
            <a
            href="https://www.dropbox.com/s/n6pq9an3avft6mh/fgm_model.zip?dl=1">The
            code to train and evaluate the model can be found here.</a>
            [5.4MB]
            </br>

          </p>
          </div>

    <hr class="soften"></hr>

          <div class="project-page">
          <a name="refs"></a>
          <h2>Reference</h2>
          <p class="lead">
          If you find this useful in your work please consider citing:
          <div class="highlight">
          <pre> <code> @inproceedings{fgm:coling14,
   title={Integrating Language and Vision to Generate Natural Language Descriptions of Videos in the Wild},
   author={Jesse Thomason and Subhashini Venugopalan and Sergio Guadarrama and Kate Saenko and Raymond Mooney},
   booktitle={Proceedings of the 25th International Conference on Computational Linguistics (COLING)},
   year={2015}
 }</code> </pre>

          </div>

    </div>
    </div> <!--container-->

    <footer class="footer">
        <div class="container">
            <p class="text-muted text-center" style="padding-top: 10px">
                <a title="Creative Commons Attribution 4.0 International license" target="_blank" href="http://creativecommons.org/licenses/by/4.0/" rel="license">
                  <img alt="License" src="imgs/cca4-88x31.png"></img>
                </a>
            </p>
        </div>
    </footer>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>
    <script src="js/toggle.js"></script>
  </body>
</html>

