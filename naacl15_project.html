<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Subhashini Venugopalan</title>

    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/template.css" rel="stylesheet">
    <script type="text/javascript"><!-- 
    function obfuscate( domain, name ) { document.write('<a href="mai' + 
    'lto:' + name + '@' + domain + '">' + name + '@' + domain + '</' + 'a>'); }
    // --></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-59771467-2', 'auto');
  ga('send', 'pageview');

</script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
          <!-- Brand and toggle get grouped for better mobile display -->
        <div class="container">

          <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
          </div>
    
          <!-- Collect the nav links, forms, and other content for toggling -->
          <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
              <li><a href="./index.html#">Home</a></li>
              <li><a href="./index.html#Research">Research</a></li>
              <li><a href="./index.html#Contact">Contact</a></li>
            </ul
          </div><!-- /.navbar-collapse -->

        </div>
    </nav>


    <div class="container">

    <div class="home-intro">
      <div class="row">
          <h1 align="center">Translating Videos to Natural Language Using Deep Recurrent Neural
          Networks </h1>
      </div>
          <div class="highlight-box">
           <h2>
           <div class="center-pills">
           <ul class="nav nav-pills">
          <li role="presentation"> <a href="#abstract">Abstract</a></li> 
          <li role="presentation"> <a href="#overview">Overview</a></li> 
          <li role="presentation"> <a href="#examples">Examples</a></li>
          <li role="presentation"> <a href="#code">Code</a></li> 
          <li role="presentation"> <a href="#refs">Reference</a></li>
           </ul>
          </div>
          </h2>
          </div>

          <div class="project-page">
          <a name="abstract"></a>
          <h2>Abstract</h2>
          <p class="text-justify">
              Solving the visual symbol grounding problem has long been a
              goal of artificial intelligence. The field appears to be
              advancing closer to this goal with recent breakthroughs in deep
              learning for natural language grounding in static images. In
              this paper, we propose to translate videos directly to
              sentences using a unified deep neural network with both
              convolutional and recurrent structure. Described video datasets
              are scarce, and most existing methods have been applied to toy
              domains with a small vocabulary of possible words. By
              transferring knowledge from 1.2M+ images with category labels
              and 100,000+ images with captions, our method is able to create
              sentence descriptions of open-domain videos with large
              vocabularies. We compare our approach with recent work using
              language generation metrics, subject, verb, and object
              prediction accuracy, and a human evaluation.
          </p>
          <center>
          <a
          href="http://www.cs.utexas.edu/users/ml/papers/venugopalan.naacl15.pdf" class="btn btn-danger" role="button">PDF</a>
          <a
          href="https://drive.google.com/file/d/0B7MS0UyZbSS8WUl3c25vX3E2bEE/view?usp=sharing" class="btn btn-success"
          role="button">Slides</a>
          <a
          href="http://techtalks.tv/talks/translating-videos-to-natural-language-using-deep-recurrent-neural-networks/61541/" class="btn btn-primary"
          role="button">Talk</a>
          </center>
          </div>

    <hr class="soften"></hr>

          <div class="project-page">
          <a name="overview"></a>
          <h2>Overview</h2>
          <center>
          <div class="approachimg">
            <img class="center-block" src="imgs/naacl-15-overview.png" alt="Mean
          Pool architecture overview"></img>
          </div>
          <p class="lead">An overview of the CNN-LSTM video to text translation model.</p>
          </center>

          </div>

    <hr class="soften"></hr>

          <div class="project-page">
          <a name="examples"></a>
          <h2>Examples</h2>
          <center>
          <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"
          src="https://www.youtube.com/embed/IGaAoW8bA4c"
          allowfullscreen></iframe>
          </div>
          <p class="lead">
          Sample Youtube clips with model output.
          </p>
          </center>
          </div>

    <hr class="soften"></hr>

          <div class="project-page">
          <a name="code"></a>
          <h2>Code</h2>
          <p class="lead">
            The code to prepare data and train the model can be found in:
            </br>
            <a
            href="https://github.com/vsubhashini/caffe/tree/recurrent/examples/youtube">
            https://github.com/vsubhashini/caffe/tree/recurrent/examples/youtube</a>
            </br>
            </br>
            Model information:<a
            href="https://gist.github.com/vsubhashini/3761b9ad43f60db9ac3d"> GitHub_Gist</a>
            </br>
            Download pre-trained model: <a
            href="https://www.dropbox.com/s/edbd49n4hhr7d7x/naacl15_pool_vgg_fc7_mean_fac2.caffemodel?dl=1">NAACL15_VGG_MEAN_POOL_MODEL</a>
            (220MB)
            </br>
            Pre-processed video data:
            <a
            href="https://www.dropbox.com/sh/4ecwl7zdha60xqo/AAC_TAsR7SkEYhkSdAFKcBlMa?dl=0">NAACL15_PRE-PROCESSED_DATA</a>

          </p>
            <h3>Notes:</h3>
          <p class="lead">
          <dl style="font-size:16px;">
            <dt>Data</dt> <dd>
            The pre-processed data contains VGG (16 layer) network's fc7 activations
            mean pooled across frames of each video, and the sentence descriptions
            accompanying all the videos. This data can be used directly with the
            code to train a network.
            </dd>
            <dt>Caffe Compatibility</dt> <dd>
            The models are currently supported by the <code>recurrent</code> branch of the
            Caffe fork
             in <a href="https://github.com/vsubhashini/caffe.git">my
             repository</a> or <a
             href="https://github.com/jdonahue/caffe.git">Jeff's
             repository</a>
            but is not yet
            compatible with the <code>master</code> branch of
            <a href="https://github.com/BVLC/caffe/">Caffe</a>.
            </dd>
            <dt>Performance</dt> <dd>
            The METEOR score of this model is 27.7% on the Youtube (MSVD) video
            test dataset.
            (refer to Table 2 in the <a href="https://arxiv.org/pdf/1505.00487">Sequence to Sequence - Video to Text
            paper</a>).
            </dd>
          </dl>
          </p>
          </div>

    <hr class="soften"></hr>

          <div class="project-page">
          <a name="refs"></a>
          <h2>Reference</h2>
          <p class="lead">
          If you find this useful in your work please consider citing:
          <div class="highlight">
          <pre> <code>@inproceedings{venugopalan:naacl15,
 title={Translating Videos to Natural Language Using Deep Recurrent Neural Networks},
 author={Venugopalan, Subhashini and Xu, Huijuan and Donahue, Jeff and
         Rohrbach, Marcus and Mooney, Raymond and Saenko, Kate},
 booktitle={{NAACL} {HLT}},
 year={2015}
}</code> </pre>

           Also consider citing <a href="http://jeffdonahue.com/lrcn/">Long-term Recurrent Convolutional Networks for Visual Recognition and Description</a>.

          </div>

    </div>
    </div> <!--container-->

    <footer class="footer">
        <div class="container">
            <p class="text-muted text-center" style="padding-top: 10px">
                <a title="Creative Commons Attribution 4.0 International license" target="_blank" href="http://creativecommons.org/licenses/by/4.0/" rel="license">
                  <img alt="License" src="imgs/cca4-88x31.png"></img>
                </a>
            </p>
        </div>
    </footer>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>
    <script src="js/toggle.js"></script>
  </body>
</html>

