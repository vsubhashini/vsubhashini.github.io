<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Subhashini Venugopalan</title>
    <!--Tracking code-->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    
      ga('create', 'UA-59771467-2', 'auto');
      ga('send', 'pageview');

    </script>

    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/template.css" rel="stylesheet">
    <script type="text/javascript"><!-- 
    function obfuscate( domain, name ) { document.write('<a href="mai' + 
    'lto:' + name + '@' + domain + '">' + name + '@' + domain + '</' + 'a>'); }
    // --></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body onload="start()">
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
          <!-- Brand and toggle get grouped for better mobile display -->
        <div class="container">

          <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
          </div>
    
          <!-- Collect the nav links, forms, and other content for toggling -->
          <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
              <li><a href="#">Home</a></li>
              <li><a href="#Research">Research</a></li>
              <li><a href="#Contact">Contact</a></li>
            </ul>
          </div><!-- /.navbar-collapse -->

        </div>
    </nav>


    <div class="container">

    <div class="home-intro">
      <div class="row">
        <div class="col-md-4 text-center" style="padding-top: 10px">
          <img class="img-circle" src="./imgs/subha_face_med.jpg"></img>
        </div>
        <div class="col-md-8">
          <h1>Subhashini Venugopalan </br><small>Research Scientist</small></h1>
          <p class="lead">
            I am an ML researcher at Google currently working on Large Language
            Model reasoning and evals. Much of my research has focused on applications
            of vision, audio, and language technologies to problems motivated in
            <a target="_blank"
              href="https://www.youtube.com/watch?v=V5aZjsWM2wo">healthcare</a>
            and sciences.
            Prior to this, I was a PhD student at UT Austin working on natural
            language processing, and computer vision.
            I was a member of the <a target="_blank"
              href="http://www.cs.utexas.edu/~ml/">Machine Learning Group</a>,
            and was advised by <a target="_blank"
              href="http://www.cs.utexas.edu/~mooney/">Prof. Ray Mooney</a>.
          </p>
          <p class="text-justify">
          During my PhD I was also fortunate to work with <a target="_blank"
          href="http://www.eecs.berkeley.edu/~trevor/">Prof. Trevor Darrell</a>'s
          group at UC Berkeley and with <a target="_blank"
          href="http://vision.cs.uml.edu/ksaenko.html">Prof. Kate Saenko</a> at
          Boston University.
          I spent several summers working on deep learning projects at
          Google Research (host: Varun Gulshan) and Google Brain (host: Andrea Frome).
           Prior to joining UT, I spent a year at IBM Research, India (Blue scholar).
          </p>
          <p class="text-justify"> In 2011, I obtained a Masters degree in Computer Science 
            and Engineering from <a target="_blank" href="http://www.iitm.ac.in/">IIT, Madras</a>. 
            My thesis was advised by Prof. C. Pandu Rangan. In 2009, I graduated with a 
            bachelor's degree in Information Technology from <a target="_blank" href="http://www.nitk.ac.in/">NITK, Surathkal</a>.
          </p>
        </div>
    </div>

    </div>
    </div> <!--container-->

    <hr class="soften"></hr>

    <div class="container">
    <a name="Research"></a>
    <div class="publications">
      <h1> Research </h1>
      <p class="lead">
        I work on Large Language Model evaluations and improving their
        reasoning. Much of my research has focused on machine learning
        applications motivated in
            <a target="_blank"
              href="https://www.youtube.com/watch?v=V5aZjsWM2wo">healthcare</a>
            and sciences. I am a key contributor to a number of works featuring in
            the
            <a target="_blank"
              href="https://www.youtube.com/watch?v=V5aZjsWM2wo">Healed through
            A.I.</a> documentary.
        Some of my work pertains to improving speech recognition systems for users
        with <a target="_blank"
                href="https://sites.research.google/euphonia/about/">impaired
                speech</a>, others to transfer learning for bio/medical data
        (e.g. detecting <a target="_blank"
                        href="https://jamanetwork.com/journals/jama/article-abstract/2588763">diabetic
                        retinopathy</a>,
                        <a target="_blank"
                        href="https://ai.googleblog.com/2017/03/assisting-pathologists-in-detecting.html">breast
                        cancer</a>), and I have also developed methods
        to interpret such vision/audio models (model explanation) for medical applications.
        During my graduate studies, I applied natural language
        processing and computer vision techniques to generate descriptions of
        events depicted in videos and images.
        Please refer to <a target="_blank"
                           href="https://scholar.google.com/citations?user=TmWYBeEAAAAJ&hl=en">my Google
                           Scholar page</a> for an up-to-date list of my publications.

      </p>
      </br>
      <h2> Talks </h2>
      </br>
        <p class="lead">
        Broader talks covering different areas of my research, usually presented
        at multiple venues.
        </p>

      <div class="twrap">
      <div class="row">
        <div class="col-md-10">
          <div class="pubt">
            <h3>
             Mar 2023 <a href="https://github.com/vsubhashini/vsubhashini.github.io/raw/master/pdfs/Prompt_Engineering_shared.pdf">
               Prompt Engineering overview</a>
          </div>
        </div>
      </div>
      </div>

      <div class="twrap">
      <div class="row">
        <div class="col-md-10">
          <div class="pubt">
            <h3>
             Aug 2022 <a
               href="https://github.com/vsubhashini/vsubhashini.github.io/raw/master/pdfs/ML_for_accessibility.pdf">
               ML for Accessibility</a>
          </div>
        </div>
      </div>
      </div>

      <div class="twrap">
      <div class="row">
        <div class="col-md-10">
          <div class="pubt">
            <h3>
             Oct 2020 <a
               href="https://github.com/vsubhashini/vsubhashini.github.io/raw/master/pdfs/XAI_for_discovery.pdf">
               Model Interpretability as a tool for discovery</a>
          </div>
        </div>
      </div>
      </div>

      </br>
      <h2> Publications </h2>
      </br>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/llm_asr_rl.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Speech Recognition With LLMs Adapted to Disordered Speech Using
              Reinforcement Learning
          </br> <small>
            Chirag Nagpal*, Subhashini Venugopalan*, Jimmy Tobin, Marilyn Ladewig,
            Katherine Heller, Katrin Tomanek
          </small></h3>
          </div>
          <div class="pubv">
            ICASSP. April 2025.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-llm-asr-rl', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://arxiv.org/abs/2412.19315">PDF</a>]</li>
	         <div id="togContent-llm-asr-rl" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 We introduce a large language model (LLM) capable of processing
                 speech inputs and show that tuning it further with
                 reinforcement learning on human preference (RLHF) enables it to
                 adapt better to disordered speech than traditional fine-tuning.
                 Our method replaces low-frequency text tokens in an LLM's
                 vocabulary with audio tokens and enables the model to recognize
                 speech by fine-tuning it on speech with transcripts. We then
                 use RL with rewards based on syntactic and semantic accuracy
                 measures generalizing the LLM further to recognize disordered
                 speech. While the resulting LLM does not outperform existing
                 systems for speech recognition, we find that tuning with
                 reinforcement learning using custom rewards leads to
                 substantially better performance than supervised fine-tuning of
                 the language model, specifically when adapting to speech in a
                 different setting. This presents a compelling alternative
                 tuning strategy for speech recognition using large language
                 models.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/usm_si_asr.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Towards a Single ASR Model That Generalizes to Disordered Speech
          </br> <small>
            Jimmy Tobin, Katrin Tomanek, Subhashini Venugopalan
          </small></h3>
          </div>
          <div class="pubv">
            ICASSP. April 2025.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-usm-siasr', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://arxiv.org/abs/2412.19315">PDF</a>]</li>
	         <div id="togContent-usm-siasr" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 This study investigates the impact of integrating a dataset of
                 disordered speech recordings (∼1,000 hours) into the
                 fine-tuning of a near state-of-the-art ASR baseline system.
                 Contrary to what one might expect, despite the data being less
                 than 1% of the training data of the ASR system, we find a
                 considerable improvement in disordered speech recognition
                 accuracy. Specifically, we observe a 33% improvement on
                 prompted speech, and a 26% improvement on a newly gathered
                 spontaneous, conversational dataset of disordered speech.
                 Importantly, there is no significant performance decline on
                 standard speech recognition benchmarks. Further, we observe
                 that the proposed tuning strategy helps close the gap between
                 the baseline system and personalized models by 64% highlighting
                 the significant progress as well as the room for improvement.
                 Given the substantial benefits of our findings, this experiment
                 suggests that from a fairness perspective, incorporating a
                 small fraction of high quality disordered speech data in a
                 training recipe is an easy step that could be done to make
                 speech technology more accessible for users with speech
                 disabilities.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>


      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/spiqa_teaser.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              SPIQA: A Dataset for Multimodal Question Answering on Scientific
              Papers
          </br> <small>
            Shraman Pramanick*, Rama Chellappa, Subhashini Venugopalan*
          </small></h3>
          </div>
          <div class="pubv">
            NeurIPS. Vancouver, Canada. December 2024.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-spiqa', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://arxiv.org/abs/2407.09413">PDF</a>]</li>
              <li>[<a
                  href="https://github.com/google/spiqa">Code</a>]</li>
              <li>[<a
                    href="https://huggingface.co/datasets/google/spiqa">Dataset</a>]</li>
	         <div id="togContent-spiqa" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Seeking answers to questions within long scientific research
                 articles is a crucial area of study that aids readers in
                 quickly addressing their inquiries. However, existing
                 question-answering (QA) datasets based on scientific papers are
                 limited in scale and focus solely on textual content. To
                 address this limitation, we introduce SPIQA (Scientific Paper
                 Image Question Answering), the first large-scale QA dataset
                 specifically designed to interpret complex figures and tables
                 within the context of scientific research articles across
                 various domains of computer science. Leveraging the breadth of
                 expertise and ability of multimodal large language models
                 (MLLMs) to understand figures, we employ automatic and manual
                 curation to create the dataset. We craft an information-seeking
                 task involving multiple images that cover a wide variety of
                 plots, charts, tables, schematic diagrams, and result
                 visualizations. SPIQA comprises 270K questions divided into
                 training, validation, and three different evaluation splits.
                 Through extensive experiments with 12 prominent foundational
                 models, we evaluate the ability of current multimodal systems
                 to comprehend the nuanced aspects of research articles.
                 Additionally, we propose a Chain-of-Thought (CoT) evaluation
                 strategy with in-context retrieval that allows fine-grained,
                 step-by-step assessment and improves model performance. We
                 further explore the upper bounds of performance enhancement
                 with additional textual information, highlighting its promising
                 potential for future research and the dataset's impact on
                 revolutionizing how we interact with scientific literature.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/skipwriter.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              SkipWriter: LLM-Powered Abbreviated Writing on Tablets
          </br> <small>
            Zheer Xu, Shanqing Cai, Mukund Varma T, Subhashini Venugopalan, Shumin Zhai
          </small></h3>
          </div>
          <div class="pubv">
           UIST. Pittsburg, USA. October 2024.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-skipwriter', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://dl.acm.org/doi/10.1145/3654777.3676423">Paper
                  </a>]</li>
              <li>[<a
                    href="https://github.com/vsubhashini/vsubhashini.github.io/raw/master/pdfs/skipwriter_poster.pdf">Poster</a>]</li>
              <li>[<a
                    href="https://github.com/vsubhashini/vsubhashini.github.io/raw/master/pdfs/skipwriter_slides.pdf">Slides</a>]</li>
	         <div id="togContent-skipwriter" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Large Language Models (LLMs) may offer transformative
                 opportunities for text input, especially for physically
                 demanding modalities like handwriting. We studied a form of
                 abbreviated handwriting by designing, developing, and
                 evaluating a prototype, named SkipWriter, that converts
                 handwritten strokes of a variable-length prefix-based
                 abbreviation (e.g. "ho a y" as handwritten strokes) into the
                 intended full phrase (e.g., "how are you" in the digital
                 format) based on the preceding context. SkipWriter consists of
                 an in-production handwriting recognizer and an LLM fine-tuned
                 on this task. With flexible pen input, SkipWriter allows the
                 user to add and revise prefix strokes when predictions do not
                 match the user's intent. An user evaluation demonstrated a 60%
                 reduction in motor movements with an average speed of 25.78
                 WPM. We also showed that this reduction is close to the ceiling
                 of our model in an offline simulation.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/chi24_design_space.jpg"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              A Design Space for Intelligent and Interactive Writing Assistants
          </br> <small>
            Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham
            Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, and others.
          </small></h3>
          </div>
          <div class="pubv">
           CHI. Hawaii, USA. May 2024.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-designspace', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://dl.acm.org/doi/full/10.1145/3613904.3642697">PDF</a>]</li>
              <li>[<a
                    href="https://www.youtube.com/watch?v=7qI2RxWKhsc">Video</a>]</li>
	         <div id="togContent-designspace" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 In our era of rapid technological advancement, the research
                 landscape for writing assistants has become increasingly
                 fragmented across various research communities. We seek to
                 address this challenge by proposing a design space as a
                 structured way to examine and explore the multidimensional
                 space of intelligent and interactive writing assistants.
                 Through community collaboration, we explore five aspects of
                 writing assistants: task, user, technology, interaction, and
                 ecosystem. Within each aspect, we define dimensions and codes
                 by systematically reviewing 115 papers, while leveraging the
                 expertise of researchers in various disciplines. Our design
                 space aims to offer researchers and designers a practical tool
                 to navigate, comprehend, and compare the various possibilities
                 of writing assistants, and aid in the design of new writing
                 assistants.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/lattescore.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Large Language Models As A Proxy For Human Evaluation In Assessing
              The Comprehensibility Of Disordered Speech Transcription
          </br> <small>
            Katrin Tomanek, Jimmy Tobin, Subhashini Venugopalan, Richard Cave,
            Katie Seaver, Jordan R. Green
          </small></h3>
          </div>
          <div class="pubv">
            ICASSP. Seoul, Korea. April 2024
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-lattescore', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://ieeexplore.ieee.org/abstract/document/10447177">PDF</a>]</li>
              <li>[<a
                  href="https://research.google/pubs/large-language-models-as-a-proxy-for-human-evaluation-in-assessing-the-comprehensibility-of-disordered-speech-transcription/">Blogpost</a>]</li>
              <li>[<a
                    href="https://github.com/vsubhashini/vsubhashini.github.io/raw/master/pdfs/lattescore_poster.pdf">Poster</a>]</li>
	         <div id="togContent-lattescore" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Automatic Speech Recognition (ASR) systems, despite significant
                 advances in recent years, still have much room for improvement
                 particularly in the recognition of disordered speech. Even so,
                 erroneous transcripts from ASR models can help people with
                 disordered speech be better understood, especially if the
                 transcription doesn’t significantly change the intended
                 meaning. Evaluating the efficacy of ASR for this use case
                 requires a methodology for measuring the impact of
                 transcription errors on the intended meaning and
                 comprehensibility. Human evaluation is the gold standard for
                 this, but it can be laborious, slow, and expensive. In this
                 work, we tune and evaluate large language models for this task
                 and find them to be a much better proxy for human evaluators
                 than other metrics commonly used. We further present a
                 case-study using the presented approach to assess the quality
                 of personalized ASR models to make model deployment decisions
                 and correctly set user expectations for model quality as part
                 of our trusted tester program.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/spice_teaser.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Speech Intelligibility Classifiers From 550K Disordered Speech Samples
          </br> <small>
            Subhashini Venugopalan, Jimmy Tobin, Samuel J. Yang, Katie Seaver, Richard J.N. Cave, Pan-Pan Jiang, Neil Zeghidour, Rus Heywood, Jordan Green, Michael P. Brenner
          </small></h3>
          </div>
          <div class="pubv">
            ICASSP. Rhodes Island, Greece. June 2023.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-spice', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10095933">PDF</a>]</li>
              <li>[<a
                  href="github.com/google-research/google-research/tree/master/euphonia_spice">Code</a>]</li>
              <li>[<a
                    href="https://github.com/vsubhashini/vsubhashini.github.io/raw/master/pdfs/SpICE-ICASSP23_slides.pdf">Slides</a>]</li>
              <li>[<a
                    href="https://github.com/vsubhashini/vsubhashini.github.io/raw/master/pdfs/SpICE_ICASSP23_Poster.pdf">Poster</a>]</li>
              <li>[<a
                    href="https://youtu.be/cRdiwTQJywQ">Video</a>]</li>
	         <div id="togContent-spice" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 We developed dysarthric speech intelligibility classifiers on
                 551,176 disordered speech samples contributed by a diverse
                 set of 468 speakers, with a range of self-reported speaking
                 disorders and rated for their overall intelligibility on a
                 fivepoint scale. We trained three models following different
                 deep
                 learning approaches and evaluated them on ∼94K utterances
                 from 100 speakers. We further found the models to generalize
                 well (without further training) on the TORGO database
                 (100% accuracy), UASpeech (0.93 correlation), ALS-TDI
                 PMP (0.81 AUC) datasets as well as on a dataset of realistic
                 unprompted speech we gathered (106 dysarthric and 76
                 control speakers, ∼2300 samples). Model will be made available
                 on request, see <a
                   href"github.com/google-research/google-research/tree/master/euphonia_spice">code
                 repo</a> for details.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>


      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/GNT.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Is Attention All That NeRF Needs?
          </br> <small>
            Mukund Varma T*, Peihao Wang*, Xuxi Chen, Tianlong Chen,
            Subhashini Venugopalan, Zhangyang Wang
          </br> *equal contribution
          </small></h3>
          </div>
          <div class="pubv">
            ICLR. Kigali, Rwanda. May 2023.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-gnt', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://github.com/VITA-Group/GNT/blob/main/docs/assets/gnt.pdf">PDF</a>]</li>
              <li>[<a
                  href="https://github.com/VITA-Group/GNT">Code</a>]</li>
              <li>[<a
                  href="https://vita-group.github.io/GNT/">Project Page</a>]</li>
	         <div id="togContent-gnt" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 We present Generalizable NeRF Transformer (GNT), a pure,
                 unified transformer-based architecture that efficiently
                 reconstructs Neural Radiance Fields (NeRFs) on the fly from
                 source views. Unlike prior works on NeRF that optimize a
                 per-scene implicit representation by inverting a handcrafted
                 rendering equation, GNT achieves generalizable neural scene
                 representation and rendering, by encapsulating two
                 transformers-based stages. The first stage of GNT, called view
                 transformer, leverages multi-view geometry as an inductive bias
                 for attention-based scene representation, and predicts
                 coordinate-aligned features by aggregating information from
                 epipolar lines on the neighboring views. The second stage of
                 GNT, named ray transformer, renders novel views by ray marching
                 and directly decodes the sequence of sampled point features
                 using the attention mechanism. Our experiments demonstrate that
                 when optimized on a single scene, GNT can successfully
                 reconstruct NeRF without explicit rendering formula, and even
                 improve the PSNR by ~1.3 dB↑ on complex scenes due to the
                 learnable ray renderer. When trained across various scenes, GNT
                 consistently achieves the state-of-the-art performance when
                 transferring to forward-facing LLFF dataset (LPIPS ~20%↓, SSIM
                 ~25%↑) and synthetic blender dataset (LPIPS ~20%↓, SSIM ~4%↑).
                 In addition, we show that depth and occlusion can be inferred
                 from the learned attention maps, which implies that the pure
                 attention mechanism is capable of learning a
                 physically-grounded rendering process. All these results bring
                 us one step closer to the tantalizing hope of utilizing
                 transformers as the "universal modeling tool" even for
                 graphics.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>


      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/speakfaster_observer2.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              SpeakFaster Observer: Long-Term Instrumentation of Eye-Gaze Typing
              for Measuring AAC Communication
          </br> <small>
            Shanqing Cai, Subhashini Venugopalan, Katrin Tomanek,
             Shaun Kane, Meredith Ringel Morris, Richard Cave, Bob
             MacDonald, Jon Campbell, Blair Casey, Emily Kornman, Daniel Vance,
             Jay Beavers
          </small></h3>
          </div>
          <div class="pubv">
            CHI. Hamburg, Germany. April 2023.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-sfobs', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://research.google/pubs/pub52025/">PDF</a>]</li>
	         <div id="togContent-sfobs" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Accelerating communication for users with severe motor and
                 speech impairments, in particular for eye-gaze Augmentative and
                 Alternative Communication (AAC) device users, is a
                 long-standing area of research. However, observation of such
                 users' communication over extended durations has been limited.
                 This case study presents the real-world experience of
                 developing and field-testing a tool for observing and curating
                 the gaze typing-based communication of a consented eye-gaze AAC
                 user with amyotrophic lateral sclerosis (ALS) from the
                 perspective of researchers at the intersection of HCI and
                 artificial intelligence (AI). With the intent to observe and
                 accelerate eye-gaze typed communication, we designed a tool and
                 a protocol called the SpeakFaster Observer to measure everyday
                 conversational text entry by the consenting gaze-typing user,
                 as well as several consenting conversation partners of the AAC
                 user. We detail the design of the Observer software and data
                 curation protocol, along with considerations for privacy
                 protection. The deployment of the data protocol from November
                 2021 to April 2022 yielded a rich dataset of gaze-based AAC
                 text entry in everyday context, consisting of 130+ hours of
                 gaze keypresses and 5.5k+ curated speech utterances from the
                 AAC user and the conversation partners. We present the key
                 statistics of the data, including the speed (8.1±3.9 words per
                 minute) and keypress saving rate (-0.18±0.87) of gaze typing,
                 patterns of of utterance repetition and reuse, as well as the
                 temporal dynamics of conversation turn-taking in gaze-based
                 communication. We share our findings and also open source our
                 data collections tools for furthering research in this domain.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/sparse_tickets.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Sparse Winning Tickets are Data-Efficient Image Recognizers
          </br> <small>
            Mukund Varma, Xuxi Chen, Zhenyu Zhang, Tianlong Chen,
            Subhashini Venugopalan, Zhangyang Wang
          </small></h3>
          </div>
          <div class="pubv">
            NeurIPS. New Orleans, USA. Dec. 2022.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-wt', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://openreview.net/forum?id=wfKbtSjHA6F">PDF</a>]</li>
              <li>[<a
                  href="https://docs.google.com/presentation/d/1gVNX23VgFRUR9e_4tHvBlMBXLg6wnMQoa_zWOri1rWM/edit?usp=sharing">Slides</a>]</li>
              <li>[<a
                  href="https://nips.cc/media/PosterPDFs/NeurIPS%202022/54192.png?t=1669487313.4348316">Poster</a>]</li>
              <li>[<a
                  href="https://github.com/VITA-Group/DataEfficientLTH">Code</a>]</li>
	         <div id="togContent-wt" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Improving performance of deep networks in data limited regimes
                 has warranted much attention. In this work, we empirically show
                 that “winning tickets” (small subnetworks) obtained via
                 magnitude pruning based on the lottery ticket hypothesis, apart
                 from being sparse are also effective recognizers in data
                 limited regimes. Based on extensive experiments, we find that
                 in low data regimes (datasets of 50-100 examples per class),
                 sparse winning tickets substantially outperform the original
                 dense networks. This approach, when combined with augmentations
                 or fine-tuning from a self-supervised backbone network, shows
                 further improvements in performance by as much as 16%
                 (absolute) on low sample datasets and longtailed
                 classification. Further, sparse winning tickets are more robust
                 to synthetic noise and distribution shifts compared to their
                 dense counterparts. Our analysis of winning tickets on small
                 datasets indicates that, though sparse, the networks retain
                 density in the initial layers and their representations are
                 more generalizable.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      </br>
      <h2> Healthcare applications, Interpretability </h2>
      </br>


      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/sf_naacl.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Context-Aware Abbreviation Expansion Using Large Language Models
          </br> <small>
            Shanqing Cai*, Subhashini Venugopalan*, Katrin Tomanek, Ajit
            Narayanan, Meredith Ringel Morris, Michael P. Brenner
          </br> *equal contribution
          </small></h3>
          </div>
          <div class="pubv">
            NAACL. Seattle, USA (+Virtual). July 2022.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-sfnaacl', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://arxiv.org/abs/2205.03767">PDF</a>]</li>
              <li>[<a
                  href="https://docs.google.com/presentation/d/1Uydpng5QgZLLM6jyLJRZi6fM5cbw9fbdfcxwN9Fv3mA/edit?usp=drivesdk">Slides</a>]</li>
	         <div id="togContent-sfnaacl" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Motivated by the need for accelerating text entry in
                 augmentative and alternative communication (AAC) for people
                 with severe motor impairments, we propose a paradigm in which
                 phrases are abbreviated aggressively as primarily word-initial
                 letters. Our approach is to
                 expand the abbreviations into full-phrase options by leveraging
                 conversation context with
                 the power of pretrained large language models (LLMs). Through
                 zero-shot, few-shot, and
                 fine-tuning experiments on four public conversation datasets,
                 we show that for replies to the
                 initial turn of a dialog, an LLM with 64B parameters is able to
                 accurately expand over 70%
                 of phrases with abbreviation length up to 10,
                 leading to an effective keystroke saving rate of
                 up to 77% on these expansions. Including a
                 small amount of context in the form of a single
                 conversation turn more than doubles abbreviation expansion
                 accuracies compared to having
                 no context, an effect that is more pronounced
                 for longer phrases. Additionally, the robustness of the models
                 against typo noise can be
                 enhanced through fine-tuning on noisy data.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/voice_cnn.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              A machine-learning based objective measure for ALS disease
              severity
          </br> <small>
            Fernando G. Vieira*, Subhashini Venugopalan*, Alan S. Premasiri, Maeve
            McNally, Aren Jansen, Kevin McCloskey, Michael P. Brenner, Steven
            Perrin
          </br> *equal contribution
          </small></h3>
          </div>
          <div class="pubv">
            npj Digital Medicine. April 2022
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-alstdi', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://www.nature.com/articles/s41746-022-00588-8">PDF</a>]</li>
	         <div id="togContent-alstdi" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Amyotrophic Lateral Sclerosis (ALS) disease severity is usually
                 measured using the subjective, questionnaire-based revised ALS
                 Functional Rating Scale (ALSFRS-R). Objective measures of
                 disease severity would be powerful tools for evaluating
                 real-world drug effectiveness, efficacy in clinical trials, and
                 for identifying participants for cohort studies. We developed a
                 machine learning (ML) based objective measure for ALS disease
                 severity based on voice samples and accelerometer measurements
                 from a four-year longitudinal dataset. 584 people living with
                 ALS consented and carried out prescribed speaking and
                 limb-based tasks. 542 participants contributed 5814 voice
                 recordings, and 350 contributed 13,009 accelerometer samples,
                 while simultaneously measuring ALSFRS-R scores. Using these
                 data, we trained ML models to predict bulbar-related and
                 limb-related ALSFRS-R scores. On the test set (n = 109
                 participants) the voice models achieved a multiclass AUC of
                 0.86 (95% CI, 0.85–0.88) on speech ALSFRS-R prediction, whereas
                 the accelerometer models achieved a median multiclass AUC of
                 0.73 on 6 limb-related functions. The correlations across
                 functions observed in self-reported ALSFRS-R scores were
                 preserved in ML-derived scores. We used these models and
                 self-reported ALSFRS-R scores to evaluate the real-world
                 effects of edaravone, a drug approved for use in ALS. In the
                 cohort of 54 test participants who received edaravone as part
                 of their usual care, the ML-derived scores were consistent with
                 the self-reported ALSFRS-R scores. At the individual level, the
                 continuous ML-derived score can capture gradual changes that
                 are absent in the integer ALSFRS-R scores. This demonstrates
                 the value of these tools for assessing disease severity and,
                 potentially, drug effects.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>


      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/fibroblast_pd.jpg"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Integrating deep learning and unbiased automated high-content
              screening to identify complex disease signatures in human
              fibroblasts.
          </br> <small>
Lauren Schiff, Bianca Migliori, Ye Chen, Deidre Carter, Caitlyn Bonilla, Jenna
Hall, Minjie Fan, Edmund Tam, Sara Ahadi, Brodie Fischbacher, Anton Geraschenko,
Christopher J. Hunter, Subhashini Venugopalan, and 30 others.
          </br>
          </small></h3>
          </div>
          <div class="pubv">
            Nature Communications. March 2022
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-pd', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://www.nature.com/articles/s41467-022-28423-4">PDF</a>]</li>
              <li>[<a
                  href="https://twitter.com/nyscf/status/1507363858884743175?cxt=HHwWjsCywa_cnespAAAA">Tweet</a>]</li>
	         <div id="togContent-pd" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Drug discovery for diseases such as Parkinson’s disease are
                 impeded by the lack of screenable cellular phenotypes. We
                 present an unbiased phenotypic profiling platform that combines
                 automated cell culture, high-content imaging, Cell Painting,
                 and deep learning. We applied this platform to primary
                 fibroblasts from 91 Parkinson’s disease patients and matched
                 healthy controls, creating the largest publicly available Cell
                 Painting image dataset to date at 48 terabytes. We use fixed
                 weights from a convolutional deep neural network trained on
                 ImageNet to generate deep embeddings from each image and train
                 machine learning models to detect morphological disease
                 phenotypes. Our platform’s robustness and sensitivity allow the
                 detection of individual-specific variation with high fidelity
                 across batches and plate layouts. Lastly, our models
                 confidently separate LRRK2 and sporadic Parkinson’s disease
                 lines from healthy controls (receiver operating characteristic
                 area under curve 0.79 (0.08 standard deviation)), supporting
                 the capacity of this platform for complex disease modeling and
                 drug screening applications.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/trillsson.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              TRILLsson: Distilled Universal Paralinguistic Speech
              Representations
          </br> <small>
            Joel Shor, Subhashini Venugopalan
          </small></h3>
          </div>
          <div class="pubv">
            INTERSPEECH 2022. Incheon, South Korea (+Virtual). September 2022.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-trillsson', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://arxiv.org/abs/2203.00236">PDF</a>]</li>
              <li>[<a
                  href="https://ai.googleblog.com/2022/03/trillsson-small-universal-speech.html">Blogpost</a>]</li>
	         <div id="togContent-trillsson" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Recent advances in self-supervision have dramatically improved
                 the quality of speech representations. However, deployment of
                 state-of-the-art embedding models on devices has been
                 restricted due to their limited public availability and large
                 resource footprint. Our work addresses these issues by publicly
                 releasing a collection of paralinguistic speech models that are
                 small and near state-of-the-art performance. Our approach is
                 based on knowledge distillation, and our models are distilled
                 on public data only. We explore different architectures and
                 thoroughly evaluate our models on the Non-Semantic Speech
                 (NOSS) benchmark. Our largest distilled model is less than 15%
                 the size of the original model (314MB vs 2.2GB), achieves over
                 96% the accuracy on 6 of 7 tasks, and is trained on 6.5% the
                 data. The smallest model is 1% in size (22MB) and achieves over
                 90% the accuracy on 6 of 7 tasks. Our models outperform the
                 open source Wav2Vec 2.0 model on 6 of 7 tasks, and our smallest
                 model outperforms the open source Wav2Vec 2.0 on both emotion
                 recognition tasks despite being 7% the size.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>


      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/etiology_dist.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Comparing Supervised Models And Learned Speech Representations For
              Classifying Intelligibility Of Disordered Speech On Selected
              Phrases
          </br> <small>
            Subhashini Venugopalan, Joel Shor, Manoj Plakal, Jimmy Tobin, Katrin
            Tomanek, Jordan Green, Michael Brenner
          </small></h3>
          </div>
          <div class="pubv">
            INTERSPEECH 2021. Brno, Czechia (+Virtual). August 2021.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-dyscls', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://arxiv.org/abs/2107.03985">PDF</a>]</li>
              <li>[<a
                  href="https://docs.google.com/presentation/d/1a1eT6Fi6YEivHkVk5lUYy27i4UI_GwxNZ1SjNtyhYKQ/edit?usp=drivesdk">Slides</a>]</li>
              <li>[<a
                  href="https://www.youtube.com/watch?v=-jXj1shc7wI">Video</a>]</li>
	         <div id="togContent-dyscls" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
Automatic classification of disordered speech can provide an objective tool
for identifying the presence and severity of speech impairment. Classification
approaches can also help identify hard-to-recognize speech samples to teach ASR
systems about the variable manifestations of impaired speech. Here, we develop
and compare different deep learning techniques to classify the intelligibility
of disordered speech on selected phrases. We collected samples from a diverse
set of 661 speakers with a variety of self-reported disorders speaking 29 words
or phrases, which were rated by speech-language pathologists for their overall
intelligibility using a five-point Likert scale. We then evaluated classifiers
developed using 3 approaches: (1) a convolutional neural network (CNN) trained
for the task, (2) classifiers trained on non-semantic speech representations
from CNNs that used an unsupervised objective [1], and (3) classifiers trained
on the acoustic (encoder) embeddings from an ASR system trained on typical
speech [2]. We found that the ASR encoder's embeddings considerably outperform
the other two on detecting and classifying disordered speech. Further analysis
shows that the ASR embeddings cluster speech by the spoken phrase, while the
non-semantic embeddings cluster speech by speaker. Also, longer phrases are
more indicative of intelligibility deficits than single words.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/finch_animation.gif"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Guided Integrated Gradients: An Adaptive Path Method for Removing
              Noise
          </br> <small>
            Andrei Kapishnikov, Subhashini Venugopalan, Besim Avci, Ben Wedin,
            Michael Terry, Tolga Bolukbasi
          </small></h3>
          </div>
          <div class="pubv">
            IEEE Conference on Computer Vision and Pattern Recognition.
            Virtual. June 2021. (CVPR 2021)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-guidedig', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://openaccess.thecvf.com/content/CVPR2021/html/Kapishnikov_Guided_Integrated_Gradients_An_Adaptive_Path_Method_for_Removing_Noise_CVPR_2021_paper.html">PDF</a>]</li>
              <li>[<a
                  href="https://pair-code.github.io/saliency/#guided-ig">Project
                Page</a>]</li>
              <li>[<a
                  href="https://github.com/PAIR-code/saliency/blob/master/docs/CVPR_Guided_IG_Poster.pdf">Poster</a>]</li>
              <li>[<a
                  href="https://github.com/PAIR-code/saliency">Code</a>]</li>
	         <div id="togContent-guidedig" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Integrated Gradients (IG) is a commonly used feature
                 attribution method for deep neural networks. While IG has many
                 desirable properties, the method often produces spurious/noisy
                 pixel attributions in regions that are not related to the
                 predicted class when applied to visual models. While this has
                 been previously noted, most existing solutions are aimed at
                 addressing the symptoms by explicitly reducing the noise in the
                 resulting attributions. In this work, we show that one of the
                 causes of the problem is the accumulation of noise along the IG
                 path. To minimize the effect of this source of noise, we
                 propose adapting the attribution path itself -- conditioning
                 the path not just on the image but also on the model being
                 explained. We introduce Adaptive Path Methods (APMs) as a
                 generalization of path methods, and Guided IG as a specific
                 instance of an APM. Empirically, Guided IG creates saliency
                 maps better aligned with the model's prediction and the input
                 image that is being explained. We show through qualitative and
                 quantitative experiments that Guided IG outperforms other,
                 related methods in nearly every experiment.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/smug.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Scaling Symbolic Methods using Gradients for Neural Model
              Explanation
          </br> <small>
            Subham Sekhar Sahoo, Subhashini Venugopalan, Li Li, Rishabh Singh,
            Patrick Riley
          </small></h3>
          </div>
          <div class="pubv">
            International Conference on Learned Representations (ICLR). May 2021.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-smug', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://arxiv.org/abs/2006.16322">PDF</a>]</li>
              <li>[<a
                  href="https://iclr.cc/media/Slides/iclr/2021/virtual(03-16-00)-03-16-00UTC-2581-scaling_symboli.pdf">Slides</a>]</li>
              <li>[<a
                  href="https://github.com/google-research/google-research/tree/master/smug_saliency">Code</a>]</li>
	         <div id="togContent-smug" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Symbolic techniques based on Satisfiability Modulo Theory (SMT)
                 solvers have been proposed for analyzing and verifying neural
                 network properties, but their usage has been fairly limited
                 owing to their poor scalability with larger networks. In this
                 work, we propose a technique for combining gradient-based
                 methods with symbolic techniques to scale such analyses and
                 demonstrate its application for model explanation. In
                 particular, we apply this technique to identify minimal regions
                 in an input that are most relevant for a neural network's
                 prediction. Our approach uses gradient information (based on
                 Integrated Gradients) to focus on a subset of neurons in the
                 first layer, which allows our technique to scale to large
                 networks. The corresponding SMT constraints encode the minimal
                 input mask discovery problem such that after masking the input,
                 the activations of the selected neurons are still above a
                 threshold. After solving for the minimal masks, our approach
                 scores the mask regions to generate a relative ordering of the
                 features within the mask. This produces a saliency map which
                 explains" where a model is looking" when making a prediction.
                 We evaluate our technique on three datasets-MNIST, ImageNet,
                 and Beer Reviews, and demonstrate both quantitatively and
                 qualitatively that the regions generated by our approach are
                 sparser and achieve higher saliency scores compared to the
                 gradient-based methods alone.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/dr_prog.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Predicting Risk of Developing Diabetic Retinopathy using Deep
              Learning
          </br> <small>
            Ashish Bora, Siva Balasubramanian, Boris Babenko, Sunny Virmani,
            Subhashini Venugopalan, Akinori Mitani, Guilherme de Oliveira
            Marinho, Jorge Cuadros, Paisan Ruamviboonsuk, Greg S Corrado, Lily
            Peng, Dale R Webster, Avinash V Varadarajan, Naama Hammel, Yun Liu,
            Pinal Bavishi
          </small></h3>
          </div>
          <div class="pubv">
            THE LANCET, Digital Health, January 2021.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-drprog', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://www.thelancet.com/journals/landig/article/PIIS2589-7500(20)30250-8/fulltext">Paper</a>]</li>
	         <div id="togContent-drprog" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Diabetic retinopathy (DR) screening is instrumental in
                      preventing blindness, but faces a scaling challenge as the
                      number of diabetic patients rises. Risk stratification for
                      the development of DR may help optimize screening
                      intervals to reduce costs while improving vision-related
                      outcomes. We created and validated two versions of a deep
                      learning system (DLS) to predict the development of
                      mild-or-worse ("Mild+") DR in diabetic patients undergoing
                      DR screening. The two versions used either three-fields or
                      a single field of color fundus photographs (CFPs) as
                      input. The training set was derived from 575,431 eyes, of
                      which 28,899 had known 2-year outcome, and the remaining
                      were used to augment the training process via multi-task
                      learning. Validation was performed on both an internal
                      validation set (set A; 7,976 eyes; 3,678 with known
                      outcome) and an external validation set (set B; 4,762
                      eyes; 2,345 with known outcome). For predicting 2-year
                      development of DR, the 3-field DLS had an area under the
                      receiver operating characteristic curve (AUC) of 0.79
                      (95%CI, 0.78-0.81) on validation set A. On validation set
                      B (which contained only a single field), the 1-field DLS's
                      AUC was 0.70 (95%CI, 0.67-0.74). The DLS was prognostic
                      even after adjusting for available risk factors (p &lt; 0.001). When added to the risk factors, the 3-field DLS improved the AUC from 0.72 (95%CI, 0.68-0.76) to 0.81 (95%CI, 0.77-0.84) in validation set A, and the 1-field DLS improved the AUC from 0.62 (95%CI, 0.58-0.66) to 0.71 (95%CI, 0.68-0.75) in validation set B. The DLSs in this study identified prognostic information for DR development from CFPs. This information is independent of and more informative than the available risk factors.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/dmeexp.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Scientific Discovery by Generating Counterfactuals using Image
              Translation
          </br> <small>
            Arunachalam Narayanaswamy*, Subhashini Venugopalan*, Dale R. Webster,
            Lily Peng, Greg Corrado, Paisan Ruamviboonsuk, Pinal Bavishi, Rory
            Sayres, Abigail Huang, Siva Balasubramanian, Michael Brenner, Philip
            Nelson, Avinash V. Varadarajan
          </br> *equal contribution
          </small></h3>
          </div>
          <div class="pubv">
            International Conference on Medical Image Computing and Computer
            Assisted Intervention
            (Lima, Peru) Virtual, October 2020. (MICCAI 2020)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-dmeexp', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://arxiv.org/abs/2007.05500">PDF</a>]</li>
              <li>[<a
              href="https://github.com/tensorflow/gan/tree/master/tensorflow_gan/examples/dme_cyclegan">Code</a>]</li>
              <li>[<a
              href="https://docs.google.com/presentation/d/1pPFFQe8nGQazh0AygiHaX7MHY6dtE7UX7p_lP2KZ2FU/edit?usp=drivesdk">Slides</a>]</li>
              <li>[<a
              href="https://youtu.be/HacjLnUqbpk">Video</a>]</li>
	         <div id="togContent-dmeexp" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Model explanation techniques play a critical role in
                 understanding the source of a model's performance and making
                 its decisions transparent. Here we investigate if explanation
                 techniques can also be used as a mechanism for scientific
                 discovery. We make three contributions: first, we propose a
                 framework to convert predictions from explanation techniques to
                 a mechanism of discovery. Second, we show how generative models
                 in combination with black-box predictors can be used to
                 generate hypotheses (without human priors) that can be
                 critically examined. Third, with these techniques we study
                 classification models for retinal images predicting Diabetic
                 Macular Edema (DME), where recent work showed that a CNN
                 trained on these images is likely learning novel features in
                 the image. We demonstrate that the proposed framework is able
                 to explain the underlying scientific mechanism, thus bridging
                 the gap between the model's performance and human
                 understanding.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/blurig.gif"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Attribution in Scale and Space
          </br> <small>
            Shawn Xu, Subhashini Venugopalan, Mukund Sundararajan
          </small></h3>
          </div>
          <div class="pubv">
            IEEE Conference on Computer Vision and Pattern Recognition,
            (Seattle, USA) Virtual June 2020. (CVPR 2020)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-blurig', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://openaccess.thecvf.com/content_CVPR_2020/html/Xu_Attribution_in_Scale_and_Space_CVPR_2020_paper.html">PDF</a>]</li>
              <li>[<a
              href="https://github.com/vsubhashini/saliency">Code</a>]</li>
              <li>[<a
              href="https://docs.google.com/presentation/d/1AmtId4MdvwcwV6GBK6eoeUpcquIv18QKFo9jhwC7xrE/edit?usp=drivesdk">Slides</a>]</li>
              <li>[<a
              href="https://www.youtube.com/watch?v=0iof_BMe1Q0">Video</a>]</li>
	         <div id="togContent-blurig" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 We study the attribution problem for deep networks applied to
                 perception tasks. For vision tasks, attribution techniques
                 attribute the prediction of a network to the pixels of the
                 input image. We propose a new technique called Blur Integrated
                 Gradients (Blur IG). This technique has several advantages over
                 other methods. First, it can tell at what scale a network
                 recognizes an object. It produces scores in the scale/frequency
                 dimension, that we find captures interesting phenomena. Second,
                 it satisfies the scale-space axioms, which imply that it
                 employs perturbations that are free of artifact. We therefore
                 produce explanations that are cleaner and consistent with the
                 operation of deep networks. Third, it eliminates the need for
                 baseline parameter for Integrated Gradients for perception
                 tasks. This is desirable because the choice of baseline has a
                 significant effect on the explanations. We compare the proposed
                 technique against previous techniques and demonstrate
                 application on three tasks: ImageNet object recognition,
                 Diabetic Retinopathy prediction, and AudioSet audio event
                 identification. Code and examples are at
                 https://github.com/PAIR-code/saliency.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/octdme.webp"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Predicting optical coherence tomography-derived diabetic macular
              edema grades from fundus photographs using deep learning
          </br> <small>
            Avinash V Varadarajan, Pinal Bavishi, Paisan Ruamviboonsuk, Peranut
            Chotcomwongse, Subhashini Venugopalan, Arunachalam Narayanaswamy,
            Jorge Cuadros, Kuniyoshi Kanai, George Bresnick, Mongkol Tadarati,
            Sukhum Silpa-Archa, Jirawut Limwattanayingyong, Variya Nganthavee,
            Joseph R Ledsam, Pearse A Keane, Greg S Corrado, Lily Peng, Dale R
            Webster
          </small></h3>
          </div>
          <div class="pubv">
            Nature Communications, 2020.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-octdme', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://www.nature.com/articles/s41467-019-13922-8">PDF</a>]</li>
	         <div id="togContent-octdme" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Center-involved diabetic macular edema (ci-DME) is a major
                 cause of vision loss. Although the gold standard for diagnosis
                 involves 3D imaging, 2D imaging by fundus photography is
                 usually used in screening settings, resulting in high
                 false-positive and false-negative calls. To address this, we
                 train a deep learning model to predict ci-DME from fundus
                 photographs, with an ROC–AUC of 0.89 (95% CI: 0.87–0.91),
                 corresponding to 85% sensitivity at 80% specificity. In
                 comparison, retinal specialists have similar sensitivities
                 (82–85%), but only half the specificity (45–50%, p &lt; 0.001).
                 Our model can also detect the presence of intraretinal fluid
                 (AUC: 0.81; 95% CI: 0.81–0.86) and subretinal fluid (AUC 0.88;
                 95% CI: 0.85–0.91). Using deep learning to make predictions via
                 simple 2D images without sophisticated 3D-imaging equipment and
                 with better than specialist performance, has broad relevance to
                 many other applications in medical imaging.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/anemia.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Detection of anaemia from retinal fundus images via deep learning
          </br> <small>
            Akinori Mitani, Abigail Huang, Subhashini Venugopalan, Greg S
            Corrado, Lily Peng, Dale R Webster, Naama Hammel, Yun Liu, Avinash V
            Varadarajan
          </small></h3>
          </div>
          <div class="pubv">
            Nature BioMedical Engineering, 2019.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-anemia', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://www.nature.com/articles/s41551-019-0487-z.epdf?author_access_token=Mb6E0T7s890wrwn-g74d7tRgN0jAjWel9jnR3ZoTv0PM60VmCAPkIZuQsecTYZaotsFV6G9aJmsrn_AQFPFU2RjouiPiJkz-OeJ5RQJ8kv85ZFtWEW_cRKjzdTuC9kzUqD9bVzmUo3TIVRhqirtaaQ%3D%3D">PDF</a>]</li>
              <li>[<a
              href="https://blog.google/technology/health/anemia-detection-retina/">Blog
              Post</a>]</li>
	         <div id="togContent-anemia" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Owing to the invasiveness of diagnostic tests for anaemia and
                 the costs associated with screening for it, the condition is
                 often undetected. Here, we show that anaemia can be detected
                 via machine-learning algorithms trained using retinal fundus
                 images, study participant metadata (including race or
                 ethnicity, age, sex and blood pressure) or the combination of
                 both data types (images and study participant metadata). In a
                 validation dataset of 11,388 study participants from the UK
                 Biobank, the metadata-only, fundus-image-only and combined
                 models predicted haemoglobin concentration (in g dl–1) with
                 mean absolute error values of 0.73 (95% confidence interval:
                 0.72–0.74), 0.67 (0.66–0.68) and 0.63 (0.62–0.64),
                 respectively, and with areas under the receiver operating
                 characteristic curve (AUC) values of 0.74 (0.71–0.76), 0.87
                 (0.85–0.89) and 0.88 (0.86–0.89), respectively. For 539 study
                 participants with self-reported diabetes, the combined model
                 predicted haemoglobin concentration with a mean absolute error
                 of 0.73 (0.68–0.78) and anaemia an AUC of 0.89 (0.85–0.93).
                 Automated anaemia screening on the basis of fundus images could
                 particularly aid patients with diabetes undergoing regular
                 retinal imaging and for whom anaemia can increase morbidity and
                 mortality risks.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/eccb.jpg"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Batch Equalization with a Generative Adversarial Network
          </br> <small>
            Wesley Wei Qian, Cassandra Xia, Subhashini Venugopalan, Arunachalam
            Narayanaswamy, Jian Peng, D Michael Ando
          </small></h3>
          </div>
          <div class="pubv">
            European Conference on Computational Biology (ECCB) 2020. </br> (Also in
            Bioinformatics.)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-eccb', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://academic.oup.com/bioinformatics/article/36/Supplement_2/i875/6055901">Paper</a>]</li>
	         <div id="togContent-eccb" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Advances in automation and imaging have made it possible to
                 capture large image datasets for experiments that span multiple
                 weeks with multiple experimental batches of data. However,
                 accurate biological comparisons across the batches is
                 challenged by the batch-to-batch variation due to
                 uncontrollable experimental noise (e.g., different stain
                 intensity or illumination conditions). To mediate the batch
                 variation (i.e. the batch effect), we developed a batch
                 equalization method that can transfer images from one batch to
                 another while preserving the biological phenotype. The
                 equalization method is trained as a generative adversarial
                 network (GAN), using the StarGAN architecture that has shown
                 considerable ability in doing style transfer for consumer
                 images. After incorporating an additional objective that
                 disentangles batch effect from biological features using an
                 existing GAN framework, we show that the equalized images have
                 less batch information as determined by a batch-prediction task
                 and perform better in a biologically relevant task (e.g.,
                 Mechanism of Action prediction).
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/confounding.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              It's easy to fool yourself: Case studies on identifying bias and
              confounding in bio-medical datasets
          </br> <small>
            Subhashini Venugopalan*, Arunachalam Narayanaswamy*, Samuel Yang*,
            Anton Gerashcenko, Scott Lipnick, Nina Makhortova, James Hawrot,
            Christine Marques, Joao Pereira, Michael Brenner, Lee Rubin, Brian
            Wainger, Marc Berndl
          </br> *equal contribution
          </small></h3>
          </div>
          <div class="pubv">
            NeurIPS Learning Meaningful Representations of Life (LMRL) Workshop,
            2019.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-confounding', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://arxiv.org/abs/1912.07661">PDF</a>]</li>
              <li>[<a
              href="https://drive.google.com/file/d/1-HY4mNNnyc767YaUso6-lFOLkhvwnf6m/view?usp=drivesdk">Poster</a>]</li>
	         <div id="togContent-confounding" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Confounding variables are a well known source of nuisance in
                 biomedical studies. They present an even greater challenge when
                 we combine them with black-box machine learning techniques that
                 operate on raw data. This work presents two case studies. In
                 one, we discovered biases arising from systematic errors in the
                 data generation process. In the other, we found a spurious
                 source of signal unrelated to the prediction task at hand. In
                 both cases, our prediction models performed well but under
                 careful examination hidden confounders and biases were
                 revealed. These are cautionary tales on the limits of using
                 machine learning techniques on raw data from scientific
                 experiments.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>


      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/slas.gif"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Applying Deep Neural Network Analysis to High-Content Image-Based Assays
          </br> <small>
            Samuel J Yang*, Scott L Lipnick*, Nina R Makhortova*, Subhashini
            Venugopalan*, Minjie Fan*, Zan Armstrong, Thorsten M Schlaeger, Liyong Deng, Wendy K Chung, Liadan O’Callaghan, Anton Geraschenko, Dosh Whye, Marc Berndl, Jon Hazard, Brian Williams, Arunachalam Narayanaswamy, D Michael Ando, Philip Nelson, Lee L Rubin
          </br> *equal contribution
          </small></h3>
          </div>
          <div class="pubv">
            SLAS DISCOVERY: Advancing Life Sciences R&D, 2019
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-slas', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://journals.sagepub.com/doi/abs/10.1177/2472555219857715">PDF</a>]</li>
              <li>[<a
              href="">Slides</a>]</li>
	         <div id="togContent-slas" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 The etiological underpinnings of many CNS disorders are not well understood. This is likely due to the fact that individual diseases aggregate numerous pathological subtypes, each associated with a complex landscape of genetic risk factors. To overcome these challenges, researchers are integrating novel data types from numerous patients, including imaging studies capturing broadly applicable features from patient-derived materials. These datasets, when combined with machine learning, potentially hold the power to elucidate the subtle patterns that stratify patients by shared pathology. In this study, we interrogated whether high-content imaging of primary skin fibroblasts, using the Cell Painting method, could reveal disease-relevant information among patients. First, we showed that technical features such as batch/plate type, plate, and location within a plate lead to detectable nuisance signals, as revealed by a pre-trained deep neural network and analysis with deep image embeddings. Using a plate design and image acquisition strategy that accounts for these variables, we performed a pilot study with 12 healthy controls and 12 subjects affected by the severe genetic neurological disorder spinal muscular atrophy (SMA), and evaluated whether a convolutional neural network (CNN) generated using a subset of the cells could distinguish disease states on cells from the remaining unseen control–SMA pair. Our results indicate that these two populations could effectively be differentiated from one another and that model selectivity is insensitive to batch/plate type. One caveat is that the samples were also largely separated by source. These findings lay a foundation for how to conduct future studies exploring diseases with more complex genetic contributions and unknown subtypes.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>


      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/pathology.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>Detecting cancer metastases on gigapixel pathology images
          </br> <small>
            Yun Liu, Krishna Gadepalli, Mohammad Norouzi, George E Dahl, Timo
            Kohlberger, Aleksey Boyko, Subhashini Venugopalan, Aleksei Timofeev,
            Philip Q Nelson, Greg S Corrado, Jason D Hipp, Lily Peng, Martin C
            Stumpe
          </small></h3>
          </div>
          <div class="pubv">
            ArXiv, 2017
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-pathology', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://arxiv.org/abs/1703.02442">PDF</a>]</li>
              <li>[<a
              href="https://ai.googleblog.com/2017/03/assisting-pathologists-in-detecting.html">Blog
              Post</a>]</li>
	         <div id="togContent-pathology" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Each year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4% of the tumors, relative to 82.7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/jama.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs 
          </br> <small>
            Varun Gulshan, Lily Peng, Marc Coram, Martin C Stumpe, Derek Wu, Arunachalam Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Tom Madams, Jorge Cuadros, Ramasamy Kim, Rajiv Raman, Philip C Nelson, Jessica L Mega, Dale R Webster
          </small></h3>
          </div>
          <div class="pubv">
            The Journal of the American Medical Association (JAMA), 2016.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-jama', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://jamanetwork.com/journals/jama/article-abstract/2588763">PDF</a>]</li>
              <li>[<a
              href="https://ai.googleblog.com/2016/11/deep-learning-for-detection-of-diabetic.html">Blog
              Post</a>]</li>
              <li>[<a
              href="https://jamanetwork.com/journals/jama/pages/editor-picks-2010-2019">Best
                of the Decade</a>]</li>
	         <div id="togContent-jama" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 <b>Question</b>  How does the performance of an automated deep learning algorithm compare with manual grading by ophthalmologists for identifying diabetic retinopathy in retinal fundus photographs?
                 </br>
                 <b>Finding</b>  In 2 validation sets of 9963 images and 1748 images, at the operating point selected for high specificity, the algorithm had 90.3% and 87.0% sensitivity and 98.1% and 98.5% specificity for detecting referable diabetic retinopathy, defined as moderate or worse diabetic retinopathy or referable macular edema by the majority decision of a panel of at least 7 US board-certified ophthalmologists. At the operating point selected for high sensitivity, the algorithm had 97.5% and 96.1% sensitivity and 93.4% and 93.9% specificity in the 2 validation sets.
                 </br>
                 <b>Meaning</b>  Deep learning algorithms had high sensitivity and specificity for detecting diabetic retinopathy and macular edema in retinal fundus photographs.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      </br>
      <h2> Vision and Language </h2>
      </br>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/noc-teaser.svg"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Captioning Images with Diverse Objects
          </br> <small>Subhashini Venugopalan, Lisa Hendricks, Marcus
          Rohrbach, Raymond Mooney, Kate Saenko, Trevor Darrell</small></h3>
          </div>
          <div class="pubv">
            IEEE Conference on Computer Vision and Pattern Recognition,
            Honolulu, Hawaii, July 2017. (CVPR 2017)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-noc', this);return
            false">+Abstract</a>] </li>
              <li>[<a href="http://arxiv.org/abs/1606.07770">arXiv</a>]</li>
              <li>[<a
              href="http://bair.berkeley.edu/blog/2017/08/08/novel-object-captioning/">Blog
              Post</a>]</li>
              <li>[<a
              href="noc.html#code">Code</a>]</li>
              <li>[<a
              href="noc.html">Project Page</a>]</li>
	         <div id="togContent-noc" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Recent captioning models are limited in their ability to scale
                 and describe concepts unseen in paired image-text corpora. We
                 propose the Novel Object Captioner (NOC), a deep visual
                 semantic captioning model that can describe a large number of
                 object categories not present in existing image-caption
                 datasets. Our model takes advantage of external sources --
                 labeled images from object recognition datasets, and semantic
                 knowledge extracted from unannotated text. We propose
                 minimizing a joint objective which can learn from these diverse
                 data sources and leverage distributional semantic embeddings,
                 enabling the model to generalize and describe novel objects
                 outside of image-caption datasets. We demonstrate that our
                 model exploits semantic information to generate captions for
                 hundreds of object categories in the ImageNet object
                 recognition dataset that are not observed in MSCOCO
                 image-caption training data, as well as many categories that
                 are observed very rarely. Both automatic evaluations and human
                 judgements show that our model considerably outperforms prior
                 work in being able to describe many more categories of objects.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>


      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/vidtext-sum.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3> Semantic Text Summarization of Long Videos
          </br> <small>Shagan Sah, Sourabh Kulhare, Allison Gray, Subhashini Venugopalan,
          Emily Prud'hommeaux, Raymond Ptucha</small></h3>
          </div>
          <div class="pubv">
            IEEE Winter Conference on Applications of Computer Vision
            Santa Rosa, California, March 2017. (WACV 2017)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-wacv', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://drive.google.com/file/d/0B7MS0UyZbSS8cjZuM1ozN2pXWDA/view?usp=sharing">PDF</a>]</li>
	         <div id="togContent-wacv" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Long videos captured by consumers are typically tied to
                 some of the most important moments of their lives, yet ironically
                 are often the least frequently watched. The time required
                 to initially retrieve and watch sections can be daunting.
                 In this work we propose novel techniques for summarizing
                 and annotating long videos. Existing video summarization
                 techniques focus exclusively on identifying keyframes
                 and subshots, however evaluating these summarized videos
                 is a challenging task. Our work proposes methods to generate
                 visual summaries of long videos, and in addition proposes
                 techniques to annotate and generate textual summaries 
                 of the videos using recurrent networks. Interesting
                 segments of long video are extracted based on image quality
                 as well as cinematographic and consumer preference. Key
                 frames from the most impactful segments are converted to
                 textual annotations using sequential encoding and decoding
                 deep learning models. Our summarization technique is
                 benchmarked on the VideoSet dataset, and evaluated by humans
                 for informative and linguistic content. We believe this
                 to be the first fully automatic method capable of simultaneous
                 visual and textual summarization of long consumer
                 videos.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/lm_fusion/lm_fusion.svg"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Improving LSTM-based Video Description with Linguistic Knowledge
           Mined from Text
          </br> <small>Subhashini Venugopalan, Lisa Hendricks,
          Raymond Mooney, Kate Saenko</small></h3>
          </div>
          <div class="pubv">
            Conference on Empirical Methods in Natural Language Processing,
            Austin, Texas, November 2016. (EMNLP 2016)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-fus', this);return
            false">+Abstract</a>] </li>
              <li>[<a href="http://arxiv.org/abs/1604.01729">PDF</a>]</li>
              <li>[<a
              href="language_fusion.html#code">Code</a>]</li>
              <li>[<a
              href="language_fusion.html">Project Page</a>]</li>
	         <div id="togContent-fus" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 This paper investigates how linguistic knowledge mined from
                 large text corpora can aid the generation of natural language
                 descriptions of videos. Specifically, we integrate both a
                 neural language model and distributional semantics trained on
                 large text corpora into a recent LSTM-based architecture for
                 video description. We evaluate our approach on a collection of
                 Youtube videos as well as two large movie description datasets
                 showing significant improvements in grammaticality while
                 modestly improving descriptive quality.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="imgs/dcc-teaser.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Deep Compositional Captioning: Describing Novel Object Categories
           without Paired Training Data
          </br> <small>Lisa Hendricks, Subhashini Venugopalan, Marcus
          Rohrbach, Raymond Mooney, Kate Saenko, Trevor Darrell</small></h3>
          </div>
          <div class="pubv">
            IEEE Conference on Computer Vision and Pattern Recognition, Las
            Vegas, Nevada, June 2016. (CVPR 2016)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-dcc', this);return
            false">+Abstract</a>] </li>
              <li>[<a href="http://arxiv.org/abs/1511.05284">arXiv</a>]</li>
              <li>[<a
              href="https://github.com/LisaAnne/DCC">Code</a>]</li>
              <li>[<a
              href="https://people.eecs.berkeley.edu/~lisa_anne/dcc_project_page.html">Project Page</a>]</li>
	         <div id="togContent-dcc" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 While recent deep neural network models have achieved promising
                 results on the image captioning task, they rely largely on the
                 availability of corpora with paired image and sentence captions
                 to describe objects in context. In this work, we propose the
                 Deep Compositional Captioner (DCC) to address the task of
                 generating descriptions of novel objects which are not present
                 in paired image-sentence datasets. Our method achieves this by
                 leveraging large object recognition datasets and external text
                 corpora and by transferring knowledge between semantically
                 similar concepts. Current deep caption models can only describe
                 objects contained in paired image-sentence corpora, despite the
                 fact that they are pre-trained with large object recognition
                 datasets, namely ImageNet. In contrast, our model can compose
                 sentences that describe novel objects and their interactions
                 with other objects. We demonstrate our model's ability to
                 describe novel concepts by empirically evaluating its
                 performance on MSCOCO and show qualitative results on ImageNet
                 images of objects for which no paired image-caption data exist.
                 Further, we extend our approach to generate descriptions of
                 objects in video clips. Our results show that DCC has distinct
                 advantages over existing image and video captioning approaches
                 for generating descriptions of new objects in context.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 10px">
          <div class="pubimg">
            <img src="imgs/iccv-teaser.jpg"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Sequence to Sequence - Video to Text
          </br> <small>Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue,
           Raymond Mooney, Trevor Darrell, Kate Saenko</small></h3>
          </div>
          <div class="pubv">
            International Conference on Computer Vision,
            Santiago, Chile, December 2015. (ICCV 2015)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-s2vt', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://www.cs.utexas.edu/users/ml/papers/venugopalan.iccv15.pdf">PDF</a>]</li>
              <li>[<a
              href="s2vt.html#code">Code</a>]</li>
              <li>[<a
              href="s2vt.html">Project Page</a>]</li>
	         <div id="togContent-s2vt" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
    Real-world videos often have complex dynamics; and methods for generating
open-domain video descriptions should be sensitive to temporal structure and
allow both input (sequence of frames) and output (sequence of words) of variable
length. To approach this problem, we propose a novel end-to-end 
sequence-to-sequence model to generate captions for videos. For this we exploit
recurrent neural networks, specifically LSTMs, which have demonstrated
state-of-the-art performance in image caption generation. 
Our LSTM model is trained on video-sentence pairs and learns to associate a
sequence of video frames to a sequence of words in order to generate a
description of the event in the video clip. Our model naturally is able to learn
the temporal structure of the sequence of frames as well as the sequence model
of the generated sentences, i.e. a language model. 
We evaluate several variants of our model that exploit different visual features on a 
standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 10px">
          <div class="pubimg">
            <img src="imgs/trans-vids.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Translating Videos to Natural Language Using Deep Recurrent Neural Networks
          </br> <small>Subhashini Venugopalan, Huijun Xu, Jeff Donahue, Marcus
          Rohrbach, Raymond Mooney, Kate Saenko</small></h3>
          </div>
          <div class="pubv">
            North American Chapter of the Association for Computational
            Linguistics, Denver, Colorado, June 2015. (NAACL-HLT 2015)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-transvid', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://www.cs.utexas.edu/users/ml/papers/venugopalan.naacl15.pdf">PDF</a>]</li>
              <li>[<a
              href="naacl15_project.html#code">Code</a>]</li>
              <li>[<a
              href="naacl15_project.html">Project Page</a>]</li>
	         <div id="togContent-transvid" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Solving the visual symbol grounding problem has long been a
                 goal of artificial intelligence. The field appears to be
                 advancing closer to this goal with recent breakthroughs in deep
                 learning for natural language grounding in static images. In
                 this paper, we propose to translate videos directly to
                 sentences using a unified deep neural network with both
                 convolutional and recurrent structure. Described video datasets
                 are scarce, and most existing methods have been applied to toy
                 domains with a small vocabulary of possible words. By
                 transferring knowledge from 1.2M+ images with category labels
                 and 100,000+ images with captions, our method is able to create
                 sentence descriptions of open-domain videos with large
                 vocabularies. We compare our approach with recent work using
                 language generation metrics, subject, verb, and object
                 prediction accuracy, and a human evaluation.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="imgs/lrcn.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Long-term Recurrent Convolutional Networks for Visual Recognition
           and Description
          </br> <small>Jeff Donahue, Lisa Hendricks, Sergio Guadarrama, Marcus
          Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell</small></h3>
          </div>
          <div class="pubv">
            IEEE Conference on Computer Vision and Pattern Recognition, Boston,
            Massachusetts, June 2015. (CVPR 2015)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-lrcn', this);return
            false">+Abstract</a>] </li>
              <li>[<a href="http://arxiv.org/pdf/1411.4389v3">PDF</a>]</li>
              <li>[<a href="http://jeffdonahue.com/lrcn/">Project page</a>]</li>
	         <div id="togContent-lrcn" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Models based on deep convolutional networks have dominated
                 recent image interpretation tasks; we investigate whether
                 models which are also recurrent, or "temporally deep", are
                 effective for tasks involving sequences, visual and otherwise.
                 We develop a novel recurrent convolutional architecture
                 suitable for large-scale visual learning which is end-to-end
                 trainable, and demonstrate the value of these models on
                 benchmark video recognition tasks, image description and
                 retrieval problems, and video narration challenges. In contrast
                 to current models which assume a fixed spatio-temporal
                 receptive field or simple temporal averaging for sequential
                 processing, recurrent convolutional models are "doubly deep"'
                 in that they can be compositional in spatial and temporal
                 "layers". Such models may have advantages when target concepts
                 are complex and/or training data are limited. Learning
                 long-term dependencies is possible when nonlinearities are
                 incorporated into the network state updates. Long-term RNN
                 models are appealing in that they directly can map
                 variable-length inputs (e.g., video frames) to variable length
                 outputs (e.g., natural language text) and can model complex
                 temporal dynamics; yet they can be optimized with
                 backpropagation. Our recurrent long-term models are directly
                 connected to modern visual convnet models and can be jointly
                 trained to simultaneously learn temporal dynamics and
                 convolutional perceptual representations. Our results show such
                 models have distinct advantages over state-of-the-art models
                 for recognition or generation which are separately defined
                     and/or optimized. 
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="imgs/nsf-fgm.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Integrating Language and Vision to Generate Natural Language Descriptions of Videos in the Wild
          </br> <small>Jesse Thomason*, Subhashini Venugopalan*, Sergio Guadarrama, 
Kate Saenko, Raymond Mooney
          </br> *equal contribution</small>
          </h3>
          </div>
          <div class="pubv">
            International Conference on Computational Linguistics, Dublin,
            Ireland, August 2014.(COLING 2014)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-fgm', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://www.cs.utexas.edu/users/ml/papers/thomason.coling14.pdf">PDF</a>]</li>
              <li>[<a
              href="fgm.html#code">Code</a>]</li>
              <li>[<a
              href="fgm.html">Project Page</a>]</li>
	         <div id="togContent-fgm" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 This paper integrates techniques in natural language processing
                 and computer vision to improve recognition and description of
                 entities and activities in real-world videos. We propose a
                 strategy for generating textual descriptions of videos by using
                 a factor graph to combine visual detections with language
                 statistics. We use state-of-the-art visual recognition systems
                 to obtain confidences on entities, activities, and scenes
                 present in the video. Our factor graph model combines these
                 detection confidences with probabilistic knowledge mined from
                 text corpora to estimate the most likely subject, verb, object,
                 and place. Results on YouTube videos show that our approach
                 improves both the joint detection of these latent, diverse
                 sentence components and the detection of some individual
                 components when compared to using the vision system alone, as
                 well as over a previous n-gram language-modeling approach. The
                 joint detection allows us to automatically generate more
                 accurate, richer sentential descriptions of videos with a wide
                 array of possible content. 
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap" style="border-bottom: none;">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="imgs/youtube2text.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>YouTube2Text: Recognizing and Describing Arbitrary Activities
           Using Semantic Hierarchies and Zero-shot Recognition
          </br> <small>Sergio Guadarrama,Niveda Krishnamoorthy, Girish
          Malkarnenkar, Subhashini Venugopalan, Raymond Mooney,
          Trevor Darrell, Kate Saenko</small></h3>
          </div>
          <div class="pubv">
            International Conference on Computer Vision, Sydney,
            Australia, December 2013.(ICCV 2013)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-y2t', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Guadarrama_YouTube2Text_Recognizing_and_2013_ICCV_paper.pdf">PDF</a>]</li>
              <li>[<a
              href="https://drive.google.com/file/d/0B7MS0UyZbSS8SDFETEh6WmxVUEE/view?usp=sharing">Poster</a>]</li>
	         <div id="togContent-y2t" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Despite a recent push towards large-scale object recognition,
                 activity recognition remains limited to narrow domains and
                 small vocabularies of actions. In this paper, we tackle the
                 challenge of recognizing and describing activities
                 "in-the-wild". We present a solution that takes a short video
                 clip and outputs a brief sentence that sums up the main
                 activity in the video, such as the actor, the action and its
                 object. Unlike previous work, our approach works on
                 out-of-domain actions: it does not require training videos of
                 the exact activity. If it cannot find an accurate prediction
                 for a pre-trained model, it finds a less specific answer that
                     is also plausible from a pragmatic standpoint. We use
                     semantic hierarchies learned from the data to help to
                     choose an appropriate level of generalization, and priors
                     learned from web-scale natural language corpora to penalize
                     unlikely combinations of actors/actions/objects; we also
                     use a web-scale language model to "fill in" novel verbs,
                     i.e. when the verb does not appear in the training set. We
                     evaluate our method on a large YouTube corpus and
                     demonstrate it is able to generate short sentence
                     descriptions of video clips better than baseline
                     approaches. 
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="showmore" id="showmorepubs">
      <img src="./imgs/double-arrow-down.png" id="arrow"/></div>
      
      <div id="morepubs">
      <hr class="soften"></hr>
      </br>

      <h2>Other Research</h2>
      <p> In the past I have researched on topics in public policy, theoretical cryptography
      and social network analysis.</p>
      </br>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4">
          <div class="pubimg">
            <img src="imgs/solarBOS-VR.jpg"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Topic based classification and pattern identification in patents
          </br> <small>Subhashini Venugopalan, Varun Rai</small></h3>
          </div>
          <div class="pubv">
            Technological Forecasting and Social Change, 2014
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-tfsc', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://www.sciencedirect.com/science/article/pii/S0040162514002923">PDF</a>]</li>
	         <div id="togContent-tfsc" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Patent classification systems and citation networks are used
                 extensively in innovation studies. However, non-unique mapping
                 of classification codes onto specific products/markets and the
                 difficulties in accurately capturing knowledge flows based just
                 on citation linkages present limitations to these conventional
                 patent analysis approaches. We present a natural language
                 processing based hierarchical technique that enables the
                 automatic identification and classification of patent datasets
                 into technology areas and sub-areas. The key novelty of our
                 technique is to use topic modeling to map patents to
                 probability distributions over real world categories/topics.
                 Accuracy and usefulness of our technique are tested on a
                 dataset of 10,201 patents in solar photovoltaics filed in the
                 United States Patent and Trademark Office (USPTO) between 2002
                 and 2013. We show that linguistic features from topic models
                 can be used to effectively identify the main technology area
                 that a patent's invention applies to. Our computational
                 experiments support the view that the topic distribution of a
                 patent offers a reduced-form representation of the knowledge
                 content in a patent. Accordingly, we suggest that this hidden
                 thematic structure in patents can be useful in studies of the
                 policy–innovation–geography nexus. To that end, we also
                 demonstrate an application of our technique for identifying
                 patterns in technological convergence.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4">
          <div class="pubimg">
            <img src="imgs/sna-retrieval.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
		  <h3> People and Entity Retrieval in Implicit Social Networks
          </br> <small> Suman K. Pathapati, Subhashini Venugopalan,
          Ashok P. Kumar, Anuradha Bhamidipaty</small></h3>
          </div>
          <div class="pubv">
          IEEE International Conference on Internet Multimedia Systems
          Architecture and Application, Bangalore, India December 2011.
          (IMSAA 2011)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-basna', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6156373">PDF</a>]</li>
	         <div id="togContent-basna" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Online social networks can be viewed as implicit real
                 world networks, that manage to capture a wealth of
                 information about heterogeneous nodes and edges, which are
                 highly interconnected. Such abundant data can be beneficial in
                 finding and retrieving relevant people and entities within
                 these networks. Effective methods of achieving this
                 can be useful in systems ranging from recommender
                 systems to people and entity discovery systems. Our main
                 contribution in this paper is the proposal of a novel localized
                 algorithm that operates on the sub graph of the social graph
                 and retrieves relevant people or entities. We also demonstrate
                 how such an algorithm can be used in large real world social
                 networks and graphs to efficiently retrieve relevant
                 people/entities.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap" style="border-bottom: none;">
      <div class="row">
        <div class="col-md-4">
          <div class="pubimg">
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
		  <h3> A New Approach to Threshold Attribute Based Signatures
          </br> <small> S. Sharmila Deva Selvi, Subhashini Venugopalan, C
          Pandu Rangan</small></h3>
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-abs', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://drive.google.com/file/d/0B7MS0UyZbSS8Q0s0LTM1N2RRNkE/view?usp=sharing">PDF</a>]</li>
	         <div id="togContent-abs" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
	This work proposes a novel approach to construct threshold attribute based signatures inspired by ring signatures. Threshold
attribute based signatures, defined by a <i>(t, n)</i> threshold predicate, ensure that the signer holds atleast
<i>t</i> out of a specified set of <i>n</i> attributes to pass the verification. Another way to look at this would be
that, the signer has atleast 1 out of the <i>(n \choose t)</i> combination of attribute sets. Thus, a different approach to
t-ABS would be to let the signer pick some <i>n'</i> sets of <i>t</i> attributes each, from the <i>(n \choose t)</i> possible sets, and
prove that (s)he has atleast one of the <i>n'</i> sets in his/her possession. In this work, we provide a flexible
threshold-ABS scheme that realizes this approach and prove it secure with the help of random oracles.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

    </div> <!--morepubs-->

    </div> <!--publications-->
    </div> <!--container-->

    <hr class="soften"></hr>

    <script>
      var more_pubs_shown = false;
      function start() {
      $("#arrow").click(function() {
        if(!more_pubs_shown) {
          $("#morepubs").slideDown('fast', function() {
            $("#arrow").attr('src', './imgs/double-arrow-up.png');
          });
          more_pubs_shown = true;
        } else {
          $("#morepubs").slideUp('fast', function() {
            $("#arrow").attr('src', './imgs/double-arrow-down.png');
          });
          more_pubs_shown = false;
        }
      });
    
      }
    </script>

    <div clas="container">
    <a name="Contact"></a>
    <div class="contact">
      <h1> Contact </h1>

     <span class="glyphicon glyphicon-envelope" aria-hidden="true"></span> <script> obfuscate('utexas.edu','vsubhashini'); </script>
     </br>
     <img src="imgs/GitHub-Mark-32px.png"><a
     href="http://www.github.com/vsubhashini">github.com/vsubhashini</a>
     </br>
     <a rel="me" href="https://sigmoid.social/@vsubhashini">
       <object data="imgs/mastodon-logo-black.svg" width="31" height="32"></object></a>
    </div>
    </div> <!--container-->
    

    <footer class="footer">
        <div class="container">
            <p class="text-muted text-center" style="padding-top: 10px">
                <a title="Creative Commons Attribution 4.0 International license" target="_blank" href="http://creativecommons.org/licenses/by/4.0/" rel="license">
                  <img alt="License" src="imgs/cca4-88x31.png"></img>
                </a>
            </p>
        </div>
    </footer>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="./js/bootstrap.min.js"></script>
    <script src="./js/toggle.js"></script>
  </body>
</html>

