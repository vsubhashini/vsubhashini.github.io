<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Subhashini Venugopalan</title>
    <!--Tracking code-->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    
      ga('create', 'UA-59771467-2', 'auto');
      ga('send', 'pageview');

    </script>

    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/template.css" rel="stylesheet">
    <script type="text/javascript"><!-- 
    function obfuscate( domain, name ) { document.write('<a href="mai' + 
    'lto:' + name + '@' + domain + '">' + name + '@' + domain + '</' + 'a>'); }
    // --></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body onload="start()">
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
          <!-- Brand and toggle get grouped for better mobile display -->
        <div class="container">

          <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
          </div>
    
          <!-- Collect the nav links, forms, and other content for toggling -->
          <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
              <li><a href="#">Home</a></li>
              <li><a href="#Research">Research</a></li>
              <li><a href="#Contact">Contact</a></li>
            </ul>
          </div><!-- /.navbar-collapse -->

        </div>
    </nav>


    <div class="container">

    <div class="home-intro">
      <div class="row">
        <div class="col-md-4 text-center" style="padding-top: 10px">
          <img class="img-circle" src="./imgs/subha_face_med.jpg"></img>
        </div>
        <div class="col-md-8">
          <h1>Subhashini Venugopalan </br><small>Computer Science Ph.D. student</small></h1>
          <p class="lead">
            I work in the intersection of natural language processing, computer vision and machine learning. 
            My advisor is <a target="_blank" href="http://www.cs.utexas.edu/~mooney/">Prof. Ray Mooney</a> and
            I'm a member of the <a target="_blank" href="http://www.cs.utexas.edu/~ml/">Machine Learning Group</a> at UT Austin.
          </p>
          <p class="text-justify">
          I am fortunate to also work with <a target="_blank"
          href="http://www.eecs.berkeley.edu/~trevor/">Prof. Trevor Darrell</a>'s
          group at UC Berkeley and with <a target="_blank"
          href="http://vision.cs.uml.edu/ksaenko.html">Prof. Kate Saenko</a> at
          Boston University.
          The last couple of summers I worked on computer vision and deep learning projects at
          Google Research (host: Varun Gulshan) and Google Brain (host: Andrea Frome). 
           Prior to joining UT, I spent a year at IBM Research, India (Blue scholar).
          </p>
          <p class="text-justify"> In 2011, I obtained a Masters degree in Computer Science 
            and Engineering from <a target="_blank" href="http://www.iitm.ac.in/">IIT, Madras</a>. 
            My thesis was advised by Prof. C. Pandu Rangan. In 2009, I graduated with a 
            bachelor's degree in Information Technology from <a target="_blank" href="http://www.nitk.ac.in/">NITK, Surathkal</a>.
          </p>
        </div>
    </div>

    </div>
    </div> <!--container-->

    <hr class="soften"></hr>

    <div class="container">
    <a name="Research"></a>
    <div class="publications">
      <h1> Research </h1>
      <p class="lead">
        I apply machine learning algorithms along with natural language
        processing and computer vision techniques to generate descriptions of events depicted in videos. 
        
      </p>
      </br>
      <h2> Publications </h2>
      </br>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/noc-teaser.svg"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Captioning Images with Diverse Objects
          </br> <small>Subhashini Venugopalan, Lisa Hendricks, Marcus
          Rohrbach, Raymond Mooney, Kate Saenko, Trevor Darrell</small></h3>
          </div>
          <div class="pubv">
            IEEE Conference on Computer Vision and Pattern Recognition,
            Honolulu, Hawaii, July 2017. (CVPR 2017)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-noc', this);return
            false">+Abstract</a>] </li>
              <li>[<a href="http://arxiv.org/abs/1606.07770">arXiv</a>]</li>
              <li>[<a
              href="http://bair.berkeley.edu/blog/2017/08/08/novel-object-captioning/">Blog
              Post</a>]</li>
              <li>[<a
              href="noc.html">Project Page</a>]</li>
	         <div id="togContent-noc" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Recent captioning models are limited in their ability to scale
                 and describe concepts unseen in paired image-text corpora. We
                 propose the Novel Object Captioner (NOC), a deep visual
                 semantic captioning model that can describe a large number of
                 object categories not present in existing image-caption
                 datasets. Our model takes advantage of external sources --
                 labeled images from object recognition datasets, and semantic
                 knowledge extracted from unannotated text. We propose
                 minimizing a joint objective which can learn from these diverse
                 data sources and leverage distributional semantic embeddings,
                 enabling the model to generalize and describe novel objects
                 outside of image-caption datasets. We demonstrate that our
                 model exploits semantic information to generate captions for
                 hundreds of object categories in the ImageNet object
                 recognition dataset that are not observed in MSCOCO
                 image-caption training data, as well as many categories that
                 are observed very rarely. Both automatic evaluations and human
                 judgements show that our model considerably outperforms prior
                 work in being able to describe many more categories of objects.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>


      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/vidtext-sum.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3> Semantic Text Summarization of Long Videos
          </br> <small>Shagan Sah, Sourabh Kulhare, Allison Gray, Subhashini Venugopalan,
          Emily Prud'hommeaux, Raymond Ptucha</small></h3>
          </div>
          <div class="pubv">
            IEEE Winter Conference on Applications of Computer Vision
            Santa Rosa, California, March 2017. (WACV 2017)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-wacv', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://drive.google.com/file/d/0B7MS0UyZbSS8cjZuM1ozN2pXWDA/view?usp=sharing">PDF</a>]</li>
	         <div id="togContent-wacv" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Long videos captured by consumers are typically tied to
                 some of the most important moments of their lives, yet ironically
                 are often the least frequently watched. The time required
                 to initially retrieve and watch sections can be daunting.
                 In this work we propose novel techniques for summarizing
                 and annotating long videos. Existing video summarization
                 techniques focus exclusively on identifying keyframes
                 and subshots, however evaluating these summarized videos
                 is a challenging task. Our work proposes methods to generate
                 visual summaries of long videos, and in addition proposes
                 techniques to annotate and generate textual summaries 
                 of the videos using recurrent networks. Interesting
                 segments of long video are extracted based on image quality
                 as well as cinematographic and consumer preference. Key
                 frames from the most impactful segments are converted to
                 textual annotations using sequential encoding and decoding
                 deep learning models. Our summarization technique is
                 benchmarked on the VideoSet dataset, and evaluated by humans
                 for informative and linguistic content. We believe this
                 to be the first fully automatic method capable of simultaneous
                 visual and textual summarization of long consumer
                 videos.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/lm_fusion/lm_fusion.svg"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Improving LSTM-based Video Description with Linguistic Knowledge
           Mined from Text
          </br> <small>Subhashini Venugopalan, Lisa Hendricks,
          Raymond Mooney, Kate Saenko</small></h3>
          </div>
          <div class="pubv">
            Conference on Empirical Methods in Natural Language Processing,
            Austin, Texas, November 2016. (EMNLP 2016)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-fus', this);return
            false">+Abstract</a>] </li>
              <li>[<a href="http://arxiv.org/abs/1604.01729">PDF</a>]</li>
              <li>[<a
              href="language_fusion.html#code">Code</a>]</li>
              <li>[<a
              href="language_fusion.html">Project Page</a>]</li>
	         <div id="togContent-fus" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 This paper investigates how linguistic knowledge mined from
                 large text corpora can aid the generation of natural language
                 descriptions of videos. Specifically, we integrate both a
                 neural language model and distributional semantics trained on
                 large text corpora into a recent LSTM-based architecture for
                 video description. We evaluate our approach on a collection of
                 Youtube videos as well as two large movie description datasets
                 showing significant improvements in grammaticality while
                 modestly improving descriptive quality.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="imgs/dcc-teaser.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Deep Compositional Captioning: Describing Novel Object Categories
           without Paired Training Data
          </br> <small>Lisa Hendricks, Subhashini Venugopalan, Marcus
          Rohrbach, Raymond Mooney, Kate Saenko, Trevor Darrell</small></h3>
          </div>
          <div class="pubv">
            IEEE Conference on Computer Vision and Pattern Recognition, Las
            Vegas, Nevada, June 2016. (CVPR 2016)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-dcc', this);return
            false">+Abstract</a>] </li>
              <li>[<a href="http://arxiv.org/abs/1511.05284">arXiv</a>]</li>
              <li>[<a
              href="https://github.com/LisaAnne/DCC">Code</a>]</li>
              <li>[<a
              href="https://people.eecs.berkeley.edu/~lisa_anne/dcc_project_page.html">Project Page</a>]</li>
	         <div id="togContent-dcc" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 While recent deep neural network models have achieved promising
                 results on the image captioning task, they rely largely on the
                 availability of corpora with paired image and sentence captions
                 to describe objects in context. In this work, we propose the
                 Deep Compositional Captioner (DCC) to address the task of
                 generating descriptions of novel objects which are not present
                 in paired image-sentence datasets. Our method achieves this by
                 leveraging large object recognition datasets and external text
                 corpora and by transferring knowledge between semantically
                 similar concepts. Current deep caption models can only describe
                 objects contained in paired image-sentence corpora, despite the
                 fact that they are pre-trained with large object recognition
                 datasets, namely ImageNet. In contrast, our model can compose
                 sentences that describe novel objects and their interactions
                 with other objects. We demonstrate our model's ability to
                 describe novel concepts by empirically evaluating its
                 performance on MSCOCO and show qualitative results on ImageNet
                 images of objects for which no paired image-caption data exist.
                 Further, we extend our approach to generate descriptions of
                 objects in video clips. Our results show that DCC has distinct
                 advantages over existing image and video captioning approaches
                 for generating descriptions of new objects in context.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 10px">
          <div class="pubimg">
            <img src="imgs/iccv-teaser.jpg"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Sequence to Sequence - Video to Text
          </br> <small>Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue,
           Raymond Mooney, Trevor Darrell, Kate Saenko</small></h3>
          </div>
          <div class="pubv">
            International Conference on Computer Vision,
            Santiago, Chile, December 2015. (ICCV 2015)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-s2vt', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://www.cs.utexas.edu/users/ml/papers/venugopalan.iccv15.pdf">PDF</a>]</li>
              <li>[<a
              href="s2vt.html#code">Code</a>]</li>
              <li>[<a
              href="s2vt.html">Project Page</a>]</li>
	         <div id="togContent-s2vt" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
    Real-world videos often have complex dynamics; and methods for generating
open-domain video descriptions should be sensitive to temporal structure and
allow both input (sequence of frames) and output (sequence of words) of variable
length. To approach this problem, we propose a novel end-to-end 
sequence-to-sequence model to generate captions for videos. For this we exploit
recurrent neural networks, specifically LSTMs, which have demonstrated
state-of-the-art performance in image caption generation. 
Our LSTM model is trained on video-sentence pairs and learns to associate a
sequence of video frames to a sequence of words in order to generate a
description of the event in the video clip. Our model naturally is able to learn
the temporal structure of the sequence of frames as well as the sequence model
of the generated sentences, i.e. a language model. 
We evaluate several variants of our model that exploit different visual features on a 
standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 10px">
          <div class="pubimg">
            <img src="imgs/trans-vids.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Translating Videos to Natural Language Using Deep Recurrent Neural Networks
          </br> <small>Subhashini Venugopalan, Huijun Xu, Jeff Donahue, Marcus
          Rohrbach, Raymond Mooney, Kate Saenko</small></h3>
          </div>
          <div class="pubv">
            North American Chapter of the Association for Computational
            Linguistics, Denver, Colorado, June 2015. (NAACL-HLT 2015)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-transvid', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://www.cs.utexas.edu/users/ml/papers/venugopalan.naacl15.pdf">PDF</a>]</li>
              <li>[<a
              href="naacl15_project.html#code">Code</a>]</li>
              <li>[<a
              href="naacl15_project.html">Project Page</a>]</li>
	         <div id="togContent-transvid" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Solving the visual symbol grounding problem has long been a
                 goal of artificial intelligence. The field appears to be
                 advancing closer to this goal with recent breakthroughs in deep
                 learning for natural language grounding in static images. In
                 this paper, we propose to translate videos directly to
                 sentences using a unified deep neural network with both
                 convolutional and recurrent structure. Described video datasets
                 are scarce, and most existing methods have been applied to toy
                 domains with a small vocabulary of possible words. By
                 transferring knowledge from 1.2M+ images with category labels
                 and 100,000+ images with captions, our method is able to create
                 sentence descriptions of open-domain videos with large
                 vocabularies. We compare our approach with recent work using
                 language generation metrics, subject, verb, and object
                 prediction accuracy, and a human evaluation.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="imgs/lrcn.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Long-term Recurrent Convolutional Networks for Visual Recognition
           and Description
          </br> <small>Jeff Donahue, Lisa Hendricks, Sergio Guadarrama, Marcus
          Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell</small></h3>
          </div>
          <div class="pubv">
            IEEE Conference on Computer Vision and Pattern Recognition, Boston,
            Massachusetts, June 2015. (CVPR 2015)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-lrcn', this);return
            false">+Abstract</a>] </li>
              <li>[<a href="http://arxiv.org/pdf/1411.4389v3">PDF</a>]</li>
              <li>[<a href="http://jeffdonahue.com/lrcn/">Project page</a>]</li>
	         <div id="togContent-lrcn" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Models based on deep convolutional networks have dominated
                 recent image interpretation tasks; we investigate whether
                 models which are also recurrent, or "temporally deep", are
                 effective for tasks involving sequences, visual and otherwise.
                 We develop a novel recurrent convolutional architecture
                 suitable for large-scale visual learning which is end-to-end
                 trainable, and demonstrate the value of these models on
                 benchmark video recognition tasks, image description and
                 retrieval problems, and video narration challenges. In contrast
                 to current models which assume a fixed spatio-temporal
                 receptive field or simple temporal averaging for sequential
                 processing, recurrent convolutional models are "doubly deep"'
                 in that they can be compositional in spatial and temporal
                 "layers". Such models may have advantages when target concepts
                 are complex and/or training data are limited. Learning
                 long-term dependencies is possible when nonlinearities are
                 incorporated into the network state updates. Long-term RNN
                 models are appealing in that they directly can map
                 variable-length inputs (e.g., video frames) to variable length
                 outputs (e.g., natural language text) and can model complex
                 temporal dynamics; yet they can be optimized with
                 backpropagation. Our recurrent long-term models are directly
                 connected to modern visual convnet models and can be jointly
                 trained to simultaneously learn temporal dynamics and
                 convolutional perceptual representations. Our results show such
                 models have distinct advantages over state-of-the-art models
                 for recognition or generation which are separately defined
                     and/or optimized. 
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="imgs/nsf-fgm.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Integrating Language and Vision to Generate Natural Language Descriptions of Videos in the Wild
          </br> <small>Jesse Thomason*, Subhashini Venugopalan*, Sergio Guadarrama, 
Kate Saenko, Raymond Mooney
          </br> *equal contribution</small>
          </h3>
          </div>
          <div class="pubv">
            International Conference on Computational Linguistics, Dublin,
            Ireland, August 2014.(COLING 2014)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-fgm', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://www.cs.utexas.edu/users/ml/papers/thomason.coling14.pdf">PDF</a>]</li>
              <li>[<a
              href="fgm.html#code">Code</a>]</li>
              <li>[<a
              href="fgm.html">Project Page</a>]</li>
	         <div id="togContent-fgm" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 This paper integrates techniques in natural language processing
                 and computer vision to improve recognition and description of
                 entities and activities in real-world videos. We propose a
                 strategy for generating textual descriptions of videos by using
                 a factor graph to combine visual detections with language
                 statistics. We use state-of-the-art visual recognition systems
                 to obtain confidences on entities, activities, and scenes
                 present in the video. Our factor graph model combines these
                 detection confidences with probabilistic knowledge mined from
                 text corpora to estimate the most likely subject, verb, object,
                 and place. Results on YouTube videos show that our approach
                 improves both the joint detection of these latent, diverse
                 sentence components and the detection of some individual
                 components when compared to using the vision system alone, as
                 well as over a previous n-gram language-modeling approach. The
                 joint detection allows us to automatically generate more
                 accurate, richer sentential descriptions of videos with a wide
                 array of possible content. 
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap" style="border-bottom: none;">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="imgs/youtube2text.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>YouTube2Text: Recognizing and Describing Arbitrary Activities
           Using Semantic Hierarchies and Zero-shot Recognition
          </br> <small>Sergio Guadarrama,Niveda Krishnamoorthy, Girish
          Malkarnenkar, Subhashini Venugopalan, Raymond Mooney,
          Trevor Darrell, Kate Saenko</small></h3>
          </div>
          <div class="pubv">
            International Conference on Computer Vision, Sydney,
            Australia, December 2013.(ICCV 2013)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-y2t', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Guadarrama_YouTube2Text_Recognizing_and_2013_ICCV_paper.pdf">PDF</a>]</li>
              <li>[<a
              href="https://drive.google.com/file/d/0B7MS0UyZbSS8SDFETEh6WmxVUEE/view?usp=sharing">Poster</a>]</li>
	         <div id="togContent-y2t" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Despite a recent push towards large-scale object recognition,
                 activity recognition remains limited to narrow domains and
                 small vocabularies of actions. In this paper, we tackle the
                 challenge of recognizing and describing activities
                 "in-the-wild". We present a solution that takes a short video
                 clip and outputs a brief sentence that sums up the main
                 activity in the video, such as the actor, the action and its
                 object. Unlike previous work, our approach works on
                 out-of-domain actions: it does not require training videos of
                 the exact activity. If it cannot find an accurate prediction
                 for a pre-trained model, it finds a less specific answer that
                     is also plausible from a pragmatic standpoint. We use
                     semantic hierarchies learned from the data to help to
                     choose an appropriate level of generalization, and priors
                     learned from web-scale natural language corpora to penalize
                     unlikely combinations of actors/actions/objects; we also
                     use a web-scale language model to "fill in" novel verbs,
                     i.e. when the verb does not appear in the training set. We
                     evaluate our method on a large YouTube corpus and
                     demonstrate it is able to generate short sentence
                     descriptions of video clips better than baseline
                     approaches. 
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="showmore" id="showmorepubs">
      <img src="./imgs/double-arrow-down.png" id="arrow"/></div>
      
      <div id="morepubs">
      <hr class="soften"></hr>
      </br>

      <h2>Other Research</h2>
      <p> In the past I have researched on topics in public policy, theoretical cryptography
      and social network analysis.</p>
      </br>
      <div class="pwrap">
      <div class="row">
        <div class="col-md-4">
          <div class="pubimg">
            <img src="imgs/solarBOS-VR.jpg"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Topic based classification and pattern identification in patents
          </br> <small>Subhashini Venugopalan, Varun Rai</small></h3>
          </div>
          <div class="pubv">
            Technological Forecasting and Social Change, 2014
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-tfsc', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://www.sciencedirect.com/science/article/pii/S0040162514002923">PDF</a>]</li>
	         <div id="togContent-tfsc" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Patent classification systems and citation networks are used
                 extensively in innovation studies. However, non-unique mapping
                 of classification codes onto specific products/markets and the
                 difficulties in accurately capturing knowledge flows based just
                 on citation linkages present limitations to these conventional
                 patent analysis approaches. We present a natural language
                 processing based hierarchical technique that enables the
                 automatic identification and classification of patent datasets
                 into technology areas and sub-areas. The key novelty of our
                 technique is to use topic modeling to map patents to
                 probability distributions over real world categories/topics.
                 Accuracy and usefulness of our technique are tested on a
                 dataset of 10,201 patents in solar photovoltaics filed in the
                 United States Patent and Trademark Office (USPTO) between 2002
                 and 2013. We show that linguistic features from topic models
                 can be used to effectively identify the main technology area
                 that a patent's invention applies to. Our computational
                 experiments support the view that the topic distribution of a
                 patent offers a reduced-form representation of the knowledge
                 content in a patent. Accordingly, we suggest that this hidden
                 thematic structure in patents can be useful in studies of the
                 policy–innovation–geography nexus. To that end, we also
                 demonstrate an application of our technique for identifying
                 patterns in technological convergence.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4">
          <div class="pubimg">
            <img src="imgs/sna-retrieval.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
		  <h3> People and Entity Retrieval in Implicit Social Networks
          </br> <small> Suman K. Pathapati, Subhashini Venugopalan,
          Ashok P. Kumar, Anuradha Bhamidipaty</small></h3>
          </div>
          <div class="pubv">
          IEEE International Conference on Internet Multimedia Systems
          Architecture and Application, Bangalore, India December 2011.
          (IMSAA 2011)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-basna', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6156373">PDF</a>]</li>
	         <div id="togContent-basna" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Online social networks can be viewed as implicit real
                 world networks, that manage to capture a wealth of
                 information about heterogeneous nodes and edges, which are
                 highly interconnected. Such abundant data can be beneficial in
                 finding and retrieving relevant people and entities within
                 these networks. Effective methods of achieving this
                 can be useful in systems ranging from recommender
                 systems to people and entity discovery systems. Our main
                 contribution in this paper is the proposal of a novel localized
                 algorithm that operates on the sub graph of the social graph
                 and retrieves relevant people or entities. We also demonstrate
                 how such an algorithm can be used in large real world social
                 networks and graphs to efficiently retrieve relevant
                 people/entities.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap" style="border-bottom: none;">
      <div class="row">
        <div class="col-md-4">
          <div class="pubimg">
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
		  <h3> A New Approach to Threshold Attribute Based Signatures
          </br> <small> S. Sharmila Deva Selvi, Subhashini Venugopalan, C
          Pandu Rangan</small></h3>
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-abs', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://drive.google.com/file/d/0B7MS0UyZbSS8Q0s0LTM1N2RRNkE/view?usp=sharing">PDF</a>]</li>
	         <div id="togContent-abs" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
	This work proposes a novel approach to construct threshold attribute based signatures inspired by ring signatures. Threshold
attribute based signatures, defined by a <i>(t, n)</i> threshold predicate, ensure that the signer holds atleast
<i>t</i> out of a specified set of <i>n</i> attributes to pass the verification. Another way to look at this would be
that, the signer has atleast 1 out of the <i>(n \choose t)</i> combination of attribute sets. Thus, a different approach to
t-ABS would be to let the signer pick some <i>n'</i> sets of <i>t</i> attributes each, from the <i>(n \choose t)</i> possible sets, and
prove that (s)he has atleast one of the <i>n'</i> sets in his/her possession. In this work, we provide a flexible
threshold-ABS scheme that realizes this approach and prove it secure with the help of random oracles.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

    </div> <!--morepubs-->

    </div> <!--publications-->
    </div> <!--container-->

    <hr class="soften"></hr>

    <script>
      var more_pubs_shown = false;
      function start() {
      $("#arrow").click(function() {
        if(!more_pubs_shown) {
          $("#morepubs").slideDown('fast', function() {
            $("#arrow").attr('src', './imgs/double-arrow-up.png');
          });
          more_pubs_shown = true;
        } else {
          $("#morepubs").slideUp('fast', function() {
            $("#arrow").attr('src', './imgs/double-arrow-down.png');
          });
          more_pubs_shown = false;
        }
      });
    
      }
    </script>

    <div clas="container">
    <a name="Contact"></a>
    <div class="contact">
      <h1> Contact </h1>

     <span class="glyphicon glyphicon-envelope" aria-hidden="true"></span> <script> obfuscate('utexas.edu','vsubhashini'); </script>
     </br>
     <img src="imgs/GitHub-Mark-32px.png"><a
     href="http://www.github.com/vsubhashini">github.com/vsubhashini</a>
    </div>
    </div> <!--container-->
    

    <footer class="footer">
        <div class="container">
            <p class="text-muted text-center" style="padding-top: 10px">
                <a title="Creative Commons Attribution 4.0 International license" target="_blank" href="http://creativecommons.org/licenses/by/4.0/" rel="license">
                  <img alt="License" src="imgs/cca4-88x31.png"></img>
                </a>
            </p>
        </div>
    </footer>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="./js/bootstrap.min.js"></script>
    <script src="./js/toggle.js"></script>
  </body>
</html>

