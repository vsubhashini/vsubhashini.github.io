<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Subhashini Venugopalan</title>
    <!--Tracking code-->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    
      ga('create', 'UA-59771467-2', 'auto');
      ga('send', 'pageview');

    </script>

    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/template.css" rel="stylesheet">
    <script type="text/javascript"><!-- 
    function obfuscate( domain, name ) { document.write('<a href="mai' + 
    'lto:' + name + '@' + domain + '">' + name + '@' + domain + '</' + 'a>'); }
    // --></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body onload="start()">
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
          <!-- Brand and toggle get grouped for better mobile display -->
        <div class="container">

          <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
          </div>
    
          <!-- Collect the nav links, forms, and other content for toggling -->
          <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
              <li><a href="#">Home</a></li>
              <li><a href="#Research">Research</a></li>
              <li><a href="#Contact">Contact</a></li>
            </ul>
          </div><!-- /.navbar-collapse -->

        </div>
    </nav>


    <div class="container">

    <div class="home-intro">
      <div class="row">
        <div class="col-md-4 text-center" style="padding-top: 10px">
          <img class="img-circle" src="./imgs/subha_face_med.jpg"></img>
        </div>
        <div class="col-md-8">
          <h1>Subhashini Venugopalan </br><small>Research Scientist</small></h1>
          <p class="lead">
            I am an ML/Vision researcher at Google working on problems in the
            intersection of biology, medicine, and machine learning.
            Prior to this, I was a PhD student at UT Austin working on natural
            language processing, and computer vision.
            I was a member of the <a target="_blank"
              href="http://www.cs.utexas.edu/~ml/">Machine Learning Group</a>,
            and was advised by <a target="_blank"
              href="http://www.cs.utexas.edu/~mooney/">Prof. Ray Mooney</a>.
          </p>
          <p class="text-justify">
          During my PhD I was also fortunate to work with <a target="_blank"
          href="http://www.eecs.berkeley.edu/~trevor/">Prof. Trevor Darrell</a>'s
          group at UC Berkeley and with <a target="_blank"
          href="http://vision.cs.uml.edu/ksaenko.html">Prof. Kate Saenko</a> at
          Boston University.
          I spent several summers working on deep learning projects at
          Google Research (host: Varun Gulshan) and Google Brain (host: Andrea Frome).
           Prior to joining UT, I spent a year at IBM Research, India (Blue scholar).
          </p>
          <p class="text-justify"> In 2011, I obtained a Masters degree in Computer Science 
            and Engineering from <a target="_blank" href="http://www.iitm.ac.in/">IIT, Madras</a>. 
            My thesis was advised by Prof. C. Pandu Rangan. In 2009, I graduated with a 
            bachelor's degree in Information Technology from <a target="_blank" href="http://www.nitk.ac.in/">NITK, Surathkal</a>.
          </p>
        </div>
    </div>

    </div>
    </div> <!--container-->

    <hr class="soften"></hr>

    <div class="container">
    <a name="Research"></a>
    <div class="publications">
      <h1> Research </h1>
      <p class="lead">
        I enjoy applying machine learning to visual, text, and audio data; in
        particular for healthcare applications.
        Some of my work pertains to transfer learning for bio/medical data
        (e.g. detecting <a target="_blank"
                        href="https://jamanetwork.com/journals/jama/article-abstract/2588763">diabetic
                        retinopathy</a>,
                        <a target="_blank"
                        href="https://ai.googleblog.com/2017/03/assisting-pathologists-in-detecting.html">breast
                        cancer</a>), and I have also developed methods
        to interpret such vision/audio models (model explanation) for medical applications.
        During my graduate studies, I applied natural language
        processing and computer vision techniques to generate descriptions of
        events depicted in videos and images.
        Please refer to <a target="_blank"
                           href="https://scholar.google.com/citations?user=TmWYBeEAAAAJ&hl=en">my Google
                           Scholar page</a> for an up-to-date list of my publications.

      </p>
      </br>
      <h2> Medical applications, Interpretability </h2>
      </br>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/finch_animation.gif"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Guided Integrated Gradients: An Adaptive Path Method for Removing
              Noise
          </br> <small>
            Andrei Kapishnikov, Subhashini Venugopalan, Besim Avci, Ben Wedin,
            Michael Terry, Tolga Bolukbasi
          </small></h3>
          </div>
          <div class="pubv">
            IEEE Conference on Computer Vision and Pattern Recognition.
            Virtual. June 2021. (CVPR 2021)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-guidedig', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://openaccess.thecvf.com/content/CVPR2021/html/Kapishnikov_Guided_Integrated_Gradients_An_Adaptive_Path_Method_for_Removing_Noise_CVPR_2021_paper.html">PDF</a>]</li>
              <li>[<a
                  href="https://pair-code.github.io/saliency/#guided-ig">Project
                Page</a>]</li>
              <li>[<a
                  href="https://github.com/PAIR-code/saliency/blob/master/docs/CVPR_Guided_IG_Poster.pdf">Poster</a>]</li>
              <li>[<a
                  href="https://github.com/PAIR-code/saliency">Code</a>]</li>
	         <div id="togContent-guidedig" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Integrated Gradients (IG) is a commonly used feature
                 attribution method for deep neural networks. While IG has many
                 desirable properties, the method often produces spurious/noisy
                 pixel attributions in regions that are not related to the
                 predicted class when applied to visual models. While this has
                 been previously noted, most existing solutions are aimed at
                 addressing the symptoms by explicitly reducing the noise in the
                 resulting attributions. In this work, we show that one of the
                 causes of the problem is the accumulation of noise along the IG
                 path. To minimize the effect of this source of noise, we
                 propose adapting the attribution path itself -- conditioning
                 the path not just on the image but also on the model being
                 explained. We introduce Adaptive Path Methods (APMs) as a
                 generalization of path methods, and Guided IG as a specific
                 instance of an APM. Empirically, Guided IG creates saliency
                 maps better aligned with the model's prediction and the input
                 image that is being explained. We show through qualitative and
                 quantitative experiments that Guided IG outperforms other,
                 related methods in nearly every experiment.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/smug.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Scaling Symbolic Methods using Gradients for Neural Model
              Explanation
          </br> <small>
            Subham Sekhar Sahoo, Subhashini Venugopalan, Li Li, Rishabh Singh,
            Patrick Riley
          </small></h3>
          </div>
          <div class="pubv">
            International Conference on Learned Representations (ICLR). May 2021.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-smug', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://arxiv.org/abs/2006.16322">PDF</a>]</li>
              <li>[<a
                  href="https://iclr.cc/media/Slides/iclr/2021/virtual(03-16-00)-03-16-00UTC-2581-scaling_symboli.pdf">Slides</a>]</li>
              <li>[<a
                  href="https://github.com/google-research/google-research/tree/master/smug_saliency">Code</a>]</li>
	         <div id="togContent-smug" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Symbolic techniques based on Satisfiability Modulo Theory (SMT)
                 solvers have been proposed for analyzing and verifying neural
                 network properties, but their usage has been fairly limited
                 owing to their poor scalability with larger networks. In this
                 work, we propose a technique for combining gradient-based
                 methods with symbolic techniques to scale such analyses and
                 demonstrate its application for model explanation. In
                 particular, we apply this technique to identify minimal regions
                 in an input that are most relevant for a neural network's
                 prediction. Our approach uses gradient information (based on
                 Integrated Gradients) to focus on a subset of neurons in the
                 first layer, which allows our technique to scale to large
                 networks. The corresponding SMT constraints encode the minimal
                 input mask discovery problem such that after masking the input,
                 the activations of the selected neurons are still above a
                 threshold. After solving for the minimal masks, our approach
                 scores the mask regions to generate a relative ordering of the
                 features within the mask. This produces a saliency map which
                 explains" where a model is looking" when making a prediction.
                 We evaluate our technique on three datasets-MNIST, ImageNet,
                 and Beer Reviews, and demonstrate both quantitatively and
                 qualitatively that the regions generated by our approach are
                 sparser and achieve higher saliency scores compared to the
                 gradient-based methods alone.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/dr_prog.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Predicting Risk of Developing Diabetic Retinopathy using Deep
              Learning
          </br> <small>
            Ashish Bora, Siva Balasubramanian, Boris Babenko, Sunny Virmani,
            Subhashini Venugopalan, Akinori Mitani, Guilherme de Oliveira
            Marinho, Jorge Cuadros, Paisan Ruamviboonsuk, Greg S Corrado, Lily
            Peng, Dale R Webster, Avinash V Varadarajan, Naama Hammel, Yun Liu,
            Pinal Bavishi
          </small></h3>
          </div>
          <div class="pubv">
            THE LANCET, Digital Health, January 2021.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-drprog', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://www.thelancet.com/journals/landig/article/PIIS2589-7500(20)30250-8/fulltext">Paper</a>]</li>
	         <div id="togContent-drprog" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Diabetic retinopathy (DR) screening is instrumental in
                      preventing blindness, but faces a scaling challenge as the
                      number of diabetic patients rises. Risk stratification for
                      the development of DR may help optimize screening
                      intervals to reduce costs while improving vision-related
                      outcomes. We created and validated two versions of a deep
                      learning system (DLS) to predict the development of
                      mild-or-worse ("Mild+") DR in diabetic patients undergoing
                      DR screening. The two versions used either three-fields or
                      a single field of color fundus photographs (CFPs) as
                      input. The training set was derived from 575,431 eyes, of
                      which 28,899 had known 2-year outcome, and the remaining
                      were used to augment the training process via multi-task
                      learning. Validation was performed on both an internal
                      validation set (set A; 7,976 eyes; 3,678 with known
                      outcome) and an external validation set (set B; 4,762
                      eyes; 2,345 with known outcome). For predicting 2-year
                      development of DR, the 3-field DLS had an area under the
                      receiver operating characteristic curve (AUC) of 0.79
                      (95%CI, 0.78-0.81) on validation set A. On validation set
                      B (which contained only a single field), the 1-field DLS's
                      AUC was 0.70 (95%CI, 0.67-0.74). The DLS was prognostic
                      even after adjusting for available risk factors (p &lt; 0.001). When added to the risk factors, the 3-field DLS improved the AUC from 0.72 (95%CI, 0.68-0.76) to 0.81 (95%CI, 0.77-0.84) in validation set A, and the 1-field DLS improved the AUC from 0.62 (95%CI, 0.58-0.66) to 0.71 (95%CI, 0.68-0.75) in validation set B. The DLSs in this study identified prognostic information for DR development from CFPs. This information is independent of and more informative than the available risk factors.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/dmeexp.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Scientific Discovery by Generating Counterfactuals using Image
              Translation
          </br> <small>
            Arunachalam Narayanaswamy*, Subhashini Venugopalan*, Dale R. Webster,
            Lily Peng, Greg Corrado, Paisan Ruamviboonsuk, Pinal Bavishi, Rory
            Sayres, Abigail Huang, Siva Balasubramanian, Michael Brenner, Philip
            Nelson, Avinash V. Varadarajan
          </br> *equal contribution
          </small></h3>
          </div>
          <div class="pubv">
            International Conference on Medical Image Computing and Computer
            Assisted Intervention
            (Lima, Peru) Virtual, October 2020. (MICCAI 2020)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-dmeexp', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://arxiv.org/abs/2007.05500">PDF</a>]</li>
              <li>[<a
              href="https://docs.google.com/presentation/d/1pPFFQe8nGQazh0AygiHaX7MHY6dtE7UX7p_lP2KZ2FU/edit?usp=drivesdk">Slides</a>]</li>
              <li>[<a
              href="https://youtu.be/HacjLnUqbpk">Video</a>]</li>
	         <div id="togContent-dmeexp" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Model explanation techniques play a critical role in
                 understanding the source of a model's performance and making
                 its decisions transparent. Here we investigate if explanation
                 techniques can also be used as a mechanism for scientific
                 discovery. We make three contributions: first, we propose a
                 framework to convert predictions from explanation techniques to
                 a mechanism of discovery. Second, we show how generative models
                 in combination with black-box predictors can be used to
                 generate hypotheses (without human priors) that can be
                 critically examined. Third, with these techniques we study
                 classification models for retinal images predicting Diabetic
                 Macular Edema (DME), where recent work showed that a CNN
                 trained on these images is likely learning novel features in
                 the image. We demonstrate that the proposed framework is able
                 to explain the underlying scientific mechanism, thus bridging
                 the gap between the model's performance and human
                 understanding.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/blurig.gif"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Attribution in Scale and Space
          </br> <small>
            Shawn Xu, Subhashini Venugopalan, Mukund Sundararajan
          </small></h3>
          </div>
          <div class="pubv">
            IEEE Conference on Computer Vision and Pattern Recognition,
            (Seattle, USA) Virtual June 2020. (CVPR 2020)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-blurig', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://openaccess.thecvf.com/content_CVPR_2020/html/Xu_Attribution_in_Scale_and_Space_CVPR_2020_paper.html">PDF</a>]</li>
              <li>[<a
              href="https://github.com/vsubhashini/saliency">Code</a>]</li>
              <li>[<a
              href="https://docs.google.com/presentation/d/1AmtId4MdvwcwV6GBK6eoeUpcquIv18QKFo9jhwC7xrE/edit?usp=drivesdk">Slides</a>]</li>
              <li>[<a
              href="https://www.youtube.com/watch?v=0iof_BMe1Q0">Video</a>]</li>
	         <div id="togContent-blurig" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 We study the attribution problem for deep networks applied to
                 perception tasks. For vision tasks, attribution techniques
                 attribute the prediction of a network to the pixels of the
                 input image. We propose a new technique called Blur Integrated
                 Gradients (Blur IG). This technique has several advantages over
                 other methods. First, it can tell at what scale a network
                 recognizes an object. It produces scores in the scale/frequency
                 dimension, that we find captures interesting phenomena. Second,
                 it satisfies the scale-space axioms, which imply that it
                 employs perturbations that are free of artifact. We therefore
                 produce explanations that are cleaner and consistent with the
                 operation of deep networks. Third, it eliminates the need for
                 baseline parameter for Integrated Gradients for perception
                 tasks. This is desirable because the choice of baseline has a
                 significant effect on the explanations. We compare the proposed
                 technique against previous techniques and demonstrate
                 application on three tasks: ImageNet object recognition,
                 Diabetic Retinopathy prediction, and AudioSet audio event
                 identification. Code and examples are at
                 https://github.com/PAIR-code/saliency.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/octdme.webp"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Predicting optical coherence tomography-derived diabetic macular
              edema grades from fundus photographs using deep learning
          </br> <small>
            Avinash V Varadarajan, Pinal Bavishi, Paisan Ruamviboonsuk, Peranut
            Chotcomwongse, Subhashini Venugopalan, Arunachalam Narayanaswamy,
            Jorge Cuadros, Kuniyoshi Kanai, George Bresnick, Mongkol Tadarati,
            Sukhum Silpa-Archa, Jirawut Limwattanayingyong, Variya Nganthavee,
            Joseph R Ledsam, Pearse A Keane, Greg S Corrado, Lily Peng, Dale R
            Webster
          </small></h3>
          </div>
          <div class="pubv">
            Nature Communications, 2020.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-octdme', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://www.nature.com/articles/s41467-019-13922-8">PDF</a>]</li>
	         <div id="togContent-octdme" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Center-involved diabetic macular edema (ci-DME) is a major
                 cause of vision loss. Although the gold standard for diagnosis
                 involves 3D imaging, 2D imaging by fundus photography is
                 usually used in screening settings, resulting in high
                 false-positive and false-negative calls. To address this, we
                 train a deep learning model to predict ci-DME from fundus
                 photographs, with an ROC–AUC of 0.89 (95% CI: 0.87–0.91),
                 corresponding to 85% sensitivity at 80% specificity. In
                 comparison, retinal specialists have similar sensitivities
                 (82–85%), but only half the specificity (45–50%, p &lt; 0.001).
                 Our model can also detect the presence of intraretinal fluid
                 (AUC: 0.81; 95% CI: 0.81–0.86) and subretinal fluid (AUC 0.88;
                 95% CI: 0.85–0.91). Using deep learning to make predictions via
                 simple 2D images without sophisticated 3D-imaging equipment and
                 with better than specialist performance, has broad relevance to
                 many other applications in medical imaging.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/anemia.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Detection of anaemia from retinal fundus images via deep learning
          </br> <small>
            Akinori Mitani, Abigail Huang, Subhashini Venugopalan, Greg S
            Corrado, Lily Peng, Dale R Webster, Naama Hammel, Yun Liu, Avinash V
            Varadarajan
          </small></h3>
          </div>
          <div class="pubv">
            Nature BioMedical Engineering, 2019.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-anemia', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://www.nature.com/articles/s41551-019-0487-z.epdf?author_access_token=Mb6E0T7s890wrwn-g74d7tRgN0jAjWel9jnR3ZoTv0PM60VmCAPkIZuQsecTYZaotsFV6G9aJmsrn_AQFPFU2RjouiPiJkz-OeJ5RQJ8kv85ZFtWEW_cRKjzdTuC9kzUqD9bVzmUo3TIVRhqirtaaQ%3D%3D">PDF</a>]</li>
              <li>[<a
              href="https://blog.google/technology/health/anemia-detection-retina/">Blog
              Post</a>]</li>
	         <div id="togContent-anemia" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Owing to the invasiveness of diagnostic tests for anaemia and
                 the costs associated with screening for it, the condition is
                 often undetected. Here, we show that anaemia can be detected
                 via machine-learning algorithms trained using retinal fundus
                 images, study participant metadata (including race or
                 ethnicity, age, sex and blood pressure) or the combination of
                 both data types (images and study participant metadata). In a
                 validation dataset of 11,388 study participants from the UK
                 Biobank, the metadata-only, fundus-image-only and combined
                 models predicted haemoglobin concentration (in g dl–1) with
                 mean absolute error values of 0.73 (95% confidence interval:
                 0.72–0.74), 0.67 (0.66–0.68) and 0.63 (0.62–0.64),
                 respectively, and with areas under the receiver operating
                 characteristic curve (AUC) values of 0.74 (0.71–0.76), 0.87
                 (0.85–0.89) and 0.88 (0.86–0.89), respectively. For 539 study
                 participants with self-reported diabetes, the combined model
                 predicted haemoglobin concentration with a mean absolute error
                 of 0.73 (0.68–0.78) and anaemia an AUC of 0.89 (0.85–0.93).
                 Automated anaemia screening on the basis of fundus images could
                 particularly aid patients with diabetes undergoing regular
                 retinal imaging and for whom anaemia can increase morbidity and
                 mortality risks.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/eccb.jpg"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Batch Equalization with a Generative Adversarial Network
          </br> <small>
            Wesley Wei Qian, Cassandra Xia, Subhashini Venugopalan, Arunachalam
            Narayanaswamy, Jian Peng, D Michael Ando
          </small></h3>
          </div>
          <div class="pubv">
            European Conference on Computational Biology (ECCB) 2020. </br> (Also in
            Bioinformatics.)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-eccb', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://academic.oup.com/bioinformatics/article/36/Supplement_2/i875/6055901">Paper</a>]</li>
	         <div id="togContent-eccb" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Advances in automation and imaging have made it possible to
                 capture large image datasets for experiments that span multiple
                 weeks with multiple experimental batches of data. However,
                 accurate biological comparisons across the batches is
                 challenged by the batch-to-batch variation due to
                 uncontrollable experimental noise (e.g., different stain
                 intensity or illumination conditions). To mediate the batch
                 variation (i.e. the batch effect), we developed a batch
                 equalization method that can transfer images from one batch to
                 another while preserving the biological phenotype. The
                 equalization method is trained as a generative adversarial
                 network (GAN), using the StarGAN architecture that has shown
                 considerable ability in doing style transfer for consumer
                 images. After incorporating an additional objective that
                 disentangles batch effect from biological features using an
                 existing GAN framework, we show that the equalized images have
                 less batch information as determined by a batch-prediction task
                 and perform better in a biologically relevant task (e.g.,
                 Mechanism of Action prediction).
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/confounding.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              It's easy to fool yourself: Case studies on identifying bias and
              confounding in bio-medical datasets
          </br> <small>
            Subhashini Venugopalan*, Arunachalam Narayanaswamy*, Samuel Yang*,
            Anton Gerashcenko, Scott Lipnick, Nina Makhortova, James Hawrot,
            Christine Marques, Joao Pereira, Michael Brenner, Lee Rubin, Brian
            Wainger, Marc Berndl
          </br> *equal contribution
          </small></h3>
          </div>
          <div class="pubv">
            NeurIPS Learning Meaningful Representations of Life (LMRL) Workshop,
            2019.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-confounding', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://arxiv.org/abs/1912.07661">PDF</a>]</li>
              <li>[<a
              href="https://drive.google.com/file/d/1-HY4mNNnyc767YaUso6-lFOLkhvwnf6m/view?usp=drivesdk">Poster</a>]</li>
	         <div id="togContent-confounding" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Confounding variables are a well known source of nuisance in
                 biomedical studies. They present an even greater challenge when
                 we combine them with black-box machine learning techniques that
                 operate on raw data. This work presents two case studies. In
                 one, we discovered biases arising from systematic errors in the
                 data generation process. In the other, we found a spurious
                 source of signal unrelated to the prediction task at hand. In
                 both cases, our prediction models performed well but under
                 careful examination hidden confounders and biases were
                 revealed. These are cautionary tales on the limits of using
                 machine learning techniques on raw data from scientific
                 experiments.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>


      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/slas.gif"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Applying Deep Neural Network Analysis to High-Content Image-Based Assays
          </br> <small>
            Samuel J Yang*, Scott L Lipnick*, Nina R Makhortova*, Subhashini
            Venugopalan*, Minjie Fan*, Zan Armstrong, Thorsten M Schlaeger, Liyong Deng, Wendy K Chung, Liadan O’Callaghan, Anton Geraschenko, Dosh Whye, Marc Berndl, Jon Hazard, Brian Williams, Arunachalam Narayanaswamy, D Michael Ando, Philip Nelson, Lee L Rubin
          </br> *equal contribution
          </small></h3>
          </div>
          <div class="pubv">
            SLAS DISCOVERY: Advancing Life Sciences R&D, 2019
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-slas', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://journals.sagepub.com/doi/abs/10.1177/2472555219857715">PDF</a>]</li>
              <li>[<a
              href="">Slides</a>]</li>
	         <div id="togContent-slas" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 The etiological underpinnings of many CNS disorders are not well understood. This is likely due to the fact that individual diseases aggregate numerous pathological subtypes, each associated with a complex landscape of genetic risk factors. To overcome these challenges, researchers are integrating novel data types from numerous patients, including imaging studies capturing broadly applicable features from patient-derived materials. These datasets, when combined with machine learning, potentially hold the power to elucidate the subtle patterns that stratify patients by shared pathology. In this study, we interrogated whether high-content imaging of primary skin fibroblasts, using the Cell Painting method, could reveal disease-relevant information among patients. First, we showed that technical features such as batch/plate type, plate, and location within a plate lead to detectable nuisance signals, as revealed by a pre-trained deep neural network and analysis with deep image embeddings. Using a plate design and image acquisition strategy that accounts for these variables, we performed a pilot study with 12 healthy controls and 12 subjects affected by the severe genetic neurological disorder spinal muscular atrophy (SMA), and evaluated whether a convolutional neural network (CNN) generated using a subset of the cells could distinguish disease states on cells from the remaining unseen control–SMA pair. Our results indicate that these two populations could effectively be differentiated from one another and that model selectivity is insensitive to batch/plate type. One caveat is that the samples were also largely separated by source. These findings lay a foundation for how to conduct future studies exploring diseases with more complex genetic contributions and unknown subtypes.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>


      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/pathology.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>Detecting cancer metastases on gigapixel pathology images
          </br> <small>
            Yun Liu, Krishna Gadepalli, Mohammad Norouzi, George E Dahl, Timo
            Kohlberger, Aleksey Boyko, Subhashini Venugopalan, Aleksei Timofeev,
            Philip Q Nelson, Greg S Corrado, Jason D Hipp, Lily Peng, Martin C
            Stumpe
          </small></h3>
          </div>
          <div class="pubv">
            ArXiv, 2017
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-pathology', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://arxiv.org/abs/1703.02442">PDF</a>]</li>
              <li>[<a
              href="https://ai.googleblog.com/2017/03/assisting-pathologists-in-detecting.html">Blog
              Post</a>]</li>
	         <div id="togContent-pathology" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Each year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4% of the tumors, relative to 82.7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/jama.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs 
          </br> <small>
            Varun Gulshan, Lily Peng, Marc Coram, Martin C Stumpe, Derek Wu, Arunachalam Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Tom Madams, Jorge Cuadros, Ramasamy Kim, Rajiv Raman, Philip C Nelson, Jessica L Mega, Dale R Webster
          </small></h3>
          </div>
          <div class="pubv">
            The Journal of the American Medical Association (JAMA), 2016.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-jama', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://jamanetwork.com/journals/jama/article-abstract/2588763">PDF</a>]</li>
              <li>[<a
              href="https://ai.googleblog.com/2016/11/deep-learning-for-detection-of-diabetic.html">Blog
              Post</a>]</li>
              <li>[<a
              href="https://jamanetwork.com/journals/jama/pages/editor-picks-2010-2019">Best
                of the Decade</a>]</li>
	         <div id="togContent-jama" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 <b>Question</b>  How does the performance of an automated deep learning algorithm compare with manual grading by ophthalmologists for identifying diabetic retinopathy in retinal fundus photographs?
                 </br>
                 <b>Finding</b>  In 2 validation sets of 9963 images and 1748 images, at the operating point selected for high specificity, the algorithm had 90.3% and 87.0% sensitivity and 98.1% and 98.5% specificity for detecting referable diabetic retinopathy, defined as moderate or worse diabetic retinopathy or referable macular edema by the majority decision of a panel of at least 7 US board-certified ophthalmologists. At the operating point selected for high sensitivity, the algorithm had 97.5% and 96.1% sensitivity and 93.4% and 93.9% specificity in the 2 validation sets.
                 </br>
                 <b>Meaning</b>  Deep learning algorithms had high sensitivity and specificity for detecting diabetic retinopathy and macular edema in retinal fundus photographs.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      </br>
      <h2> Vision and Language </h2>
      </br>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/noc-teaser.svg"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Captioning Images with Diverse Objects
          </br> <small>Subhashini Venugopalan, Lisa Hendricks, Marcus
          Rohrbach, Raymond Mooney, Kate Saenko, Trevor Darrell</small></h3>
          </div>
          <div class="pubv">
            IEEE Conference on Computer Vision and Pattern Recognition,
            Honolulu, Hawaii, July 2017. (CVPR 2017)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-noc', this);return
            false">+Abstract</a>] </li>
              <li>[<a href="http://arxiv.org/abs/1606.07770">arXiv</a>]</li>
              <li>[<a
              href="http://bair.berkeley.edu/blog/2017/08/08/novel-object-captioning/">Blog
              Post</a>]</li>
              <li>[<a
              href="noc.html#code">Code</a>]</li>
              <li>[<a
              href="noc.html">Project Page</a>]</li>
	         <div id="togContent-noc" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Recent captioning models are limited in their ability to scale
                 and describe concepts unseen in paired image-text corpora. We
                 propose the Novel Object Captioner (NOC), a deep visual
                 semantic captioning model that can describe a large number of
                 object categories not present in existing image-caption
                 datasets. Our model takes advantage of external sources --
                 labeled images from object recognition datasets, and semantic
                 knowledge extracted from unannotated text. We propose
                 minimizing a joint objective which can learn from these diverse
                 data sources and leverage distributional semantic embeddings,
                 enabling the model to generalize and describe novel objects
                 outside of image-caption datasets. We demonstrate that our
                 model exploits semantic information to generate captions for
                 hundreds of object categories in the ImageNet object
                 recognition dataset that are not observed in MSCOCO
                 image-caption training data, as well as many categories that
                 are observed very rarely. Both automatic evaluations and human
                 judgements show that our model considerably outperforms prior
                 work in being able to describe many more categories of objects.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>


      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/vidtext-sum.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3> Semantic Text Summarization of Long Videos
          </br> <small>Shagan Sah, Sourabh Kulhare, Allison Gray, Subhashini Venugopalan,
          Emily Prud'hommeaux, Raymond Ptucha</small></h3>
          </div>
          <div class="pubv">
            IEEE Winter Conference on Applications of Computer Vision
            Santa Rosa, California, March 2017. (WACV 2017)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-wacv', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://drive.google.com/file/d/0B7MS0UyZbSS8cjZuM1ozN2pXWDA/view?usp=sharing">PDF</a>]</li>
	         <div id="togContent-wacv" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Long videos captured by consumers are typically tied to
                 some of the most important moments of their lives, yet ironically
                 are often the least frequently watched. The time required
                 to initially retrieve and watch sections can be daunting.
                 In this work we propose novel techniques for summarizing
                 and annotating long videos. Existing video summarization
                 techniques focus exclusively on identifying keyframes
                 and subshots, however evaluating these summarized videos
                 is a challenging task. Our work proposes methods to generate
                 visual summaries of long videos, and in addition proposes
                 techniques to annotate and generate textual summaries 
                 of the videos using recurrent networks. Interesting
                 segments of long video are extracted based on image quality
                 as well as cinematographic and consumer preference. Key
                 frames from the most impactful segments are converted to
                 textual annotations using sequential encoding and decoding
                 deep learning models. Our summarization technique is
                 benchmarked on the VideoSet dataset, and evaluated by humans
                 for informative and linguistic content. We believe this
                 to be the first fully automatic method capable of simultaneous
                 visual and textual summarization of long consumer
                 videos.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/lm_fusion/lm_fusion.svg"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Improving LSTM-based Video Description with Linguistic Knowledge
           Mined from Text
          </br> <small>Subhashini Venugopalan, Lisa Hendricks,
          Raymond Mooney, Kate Saenko</small></h3>
          </div>
          <div class="pubv">
            Conference on Empirical Methods in Natural Language Processing,
            Austin, Texas, November 2016. (EMNLP 2016)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-fus', this);return
            false">+Abstract</a>] </li>
              <li>[<a href="http://arxiv.org/abs/1604.01729">PDF</a>]</li>
              <li>[<a
              href="language_fusion.html#code">Code</a>]</li>
              <li>[<a
              href="language_fusion.html">Project Page</a>]</li>
	         <div id="togContent-fus" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 This paper investigates how linguistic knowledge mined from
                 large text corpora can aid the generation of natural language
                 descriptions of videos. Specifically, we integrate both a
                 neural language model and distributional semantics trained on
                 large text corpora into a recent LSTM-based architecture for
                 video description. We evaluate our approach on a collection of
                 Youtube videos as well as two large movie description datasets
                 showing significant improvements in grammaticality while
                 modestly improving descriptive quality.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="imgs/dcc-teaser.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Deep Compositional Captioning: Describing Novel Object Categories
           without Paired Training Data
          </br> <small>Lisa Hendricks, Subhashini Venugopalan, Marcus
          Rohrbach, Raymond Mooney, Kate Saenko, Trevor Darrell</small></h3>
          </div>
          <div class="pubv">
            IEEE Conference on Computer Vision and Pattern Recognition, Las
            Vegas, Nevada, June 2016. (CVPR 2016)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-dcc', this);return
            false">+Abstract</a>] </li>
              <li>[<a href="http://arxiv.org/abs/1511.05284">arXiv</a>]</li>
              <li>[<a
              href="https://github.com/LisaAnne/DCC">Code</a>]</li>
              <li>[<a
              href="https://people.eecs.berkeley.edu/~lisa_anne/dcc_project_page.html">Project Page</a>]</li>
	         <div id="togContent-dcc" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 While recent deep neural network models have achieved promising
                 results on the image captioning task, they rely largely on the
                 availability of corpora with paired image and sentence captions
                 to describe objects in context. In this work, we propose the
                 Deep Compositional Captioner (DCC) to address the task of
                 generating descriptions of novel objects which are not present
                 in paired image-sentence datasets. Our method achieves this by
                 leveraging large object recognition datasets and external text
                 corpora and by transferring knowledge between semantically
                 similar concepts. Current deep caption models can only describe
                 objects contained in paired image-sentence corpora, despite the
                 fact that they are pre-trained with large object recognition
                 datasets, namely ImageNet. In contrast, our model can compose
                 sentences that describe novel objects and their interactions
                 with other objects. We demonstrate our model's ability to
                 describe novel concepts by empirically evaluating its
                 performance on MSCOCO and show qualitative results on ImageNet
                 images of objects for which no paired image-caption data exist.
                 Further, we extend our approach to generate descriptions of
                 objects in video clips. Our results show that DCC has distinct
                 advantages over existing image and video captioning approaches
                 for generating descriptions of new objects in context.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 10px">
          <div class="pubimg">
            <img src="imgs/iccv-teaser.jpg"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Sequence to Sequence - Video to Text
          </br> <small>Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue,
           Raymond Mooney, Trevor Darrell, Kate Saenko</small></h3>
          </div>
          <div class="pubv">
            International Conference on Computer Vision,
            Santiago, Chile, December 2015. (ICCV 2015)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-s2vt', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://www.cs.utexas.edu/users/ml/papers/venugopalan.iccv15.pdf">PDF</a>]</li>
              <li>[<a
              href="s2vt.html#code">Code</a>]</li>
              <li>[<a
              href="s2vt.html">Project Page</a>]</li>
	         <div id="togContent-s2vt" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
    Real-world videos often have complex dynamics; and methods for generating
open-domain video descriptions should be sensitive to temporal structure and
allow both input (sequence of frames) and output (sequence of words) of variable
length. To approach this problem, we propose a novel end-to-end 
sequence-to-sequence model to generate captions for videos. For this we exploit
recurrent neural networks, specifically LSTMs, which have demonstrated
state-of-the-art performance in image caption generation. 
Our LSTM model is trained on video-sentence pairs and learns to associate a
sequence of video frames to a sequence of words in order to generate a
description of the event in the video clip. Our model naturally is able to learn
the temporal structure of the sequence of frames as well as the sequence model
of the generated sentences, i.e. a language model. 
We evaluate several variants of our model that exploit different visual features on a 
standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 10px">
          <div class="pubimg">
            <img src="imgs/trans-vids.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Translating Videos to Natural Language Using Deep Recurrent Neural Networks
          </br> <small>Subhashini Venugopalan, Huijun Xu, Jeff Donahue, Marcus
          Rohrbach, Raymond Mooney, Kate Saenko</small></h3>
          </div>
          <div class="pubv">
            North American Chapter of the Association for Computational
            Linguistics, Denver, Colorado, June 2015. (NAACL-HLT 2015)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-transvid', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://www.cs.utexas.edu/users/ml/papers/venugopalan.naacl15.pdf">PDF</a>]</li>
              <li>[<a
              href="naacl15_project.html#code">Code</a>]</li>
              <li>[<a
              href="naacl15_project.html">Project Page</a>]</li>
	         <div id="togContent-transvid" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Solving the visual symbol grounding problem has long been a
                 goal of artificial intelligence. The field appears to be
                 advancing closer to this goal with recent breakthroughs in deep
                 learning for natural language grounding in static images. In
                 this paper, we propose to translate videos directly to
                 sentences using a unified deep neural network with both
                 convolutional and recurrent structure. Described video datasets
                 are scarce, and most existing methods have been applied to toy
                 domains with a small vocabulary of possible words. By
                 transferring knowledge from 1.2M+ images with category labels
                 and 100,000+ images with captions, our method is able to create
                 sentence descriptions of open-domain videos with large
                 vocabularies. We compare our approach with recent work using
                 language generation metrics, subject, verb, and object
                 prediction accuracy, and a human evaluation.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="imgs/lrcn.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Long-term Recurrent Convolutional Networks for Visual Recognition
           and Description
          </br> <small>Jeff Donahue, Lisa Hendricks, Sergio Guadarrama, Marcus
          Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell</small></h3>
          </div>
          <div class="pubv">
            IEEE Conference on Computer Vision and Pattern Recognition, Boston,
            Massachusetts, June 2015. (CVPR 2015)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-lrcn', this);return
            false">+Abstract</a>] </li>
              <li>[<a href="http://arxiv.org/pdf/1411.4389v3">PDF</a>]</li>
              <li>[<a href="http://jeffdonahue.com/lrcn/">Project page</a>]</li>
	         <div id="togContent-lrcn" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Models based on deep convolutional networks have dominated
                 recent image interpretation tasks; we investigate whether
                 models which are also recurrent, or "temporally deep", are
                 effective for tasks involving sequences, visual and otherwise.
                 We develop a novel recurrent convolutional architecture
                 suitable for large-scale visual learning which is end-to-end
                 trainable, and demonstrate the value of these models on
                 benchmark video recognition tasks, image description and
                 retrieval problems, and video narration challenges. In contrast
                 to current models which assume a fixed spatio-temporal
                 receptive field or simple temporal averaging for sequential
                 processing, recurrent convolutional models are "doubly deep"'
                 in that they can be compositional in spatial and temporal
                 "layers". Such models may have advantages when target concepts
                 are complex and/or training data are limited. Learning
                 long-term dependencies is possible when nonlinearities are
                 incorporated into the network state updates. Long-term RNN
                 models are appealing in that they directly can map
                 variable-length inputs (e.g., video frames) to variable length
                 outputs (e.g., natural language text) and can model complex
                 temporal dynamics; yet they can be optimized with
                 backpropagation. Our recurrent long-term models are directly
                 connected to modern visual convnet models and can be jointly
                 trained to simultaneously learn temporal dynamics and
                 convolutional perceptual representations. Our results show such
                 models have distinct advantages over state-of-the-art models
                 for recognition or generation which are separately defined
                     and/or optimized. 
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="imgs/nsf-fgm.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Integrating Language and Vision to Generate Natural Language Descriptions of Videos in the Wild
          </br> <small>Jesse Thomason*, Subhashini Venugopalan*, Sergio Guadarrama, 
Kate Saenko, Raymond Mooney
          </br> *equal contribution</small>
          </h3>
          </div>
          <div class="pubv">
            International Conference on Computational Linguistics, Dublin,
            Ireland, August 2014.(COLING 2014)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-fgm', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://www.cs.utexas.edu/users/ml/papers/thomason.coling14.pdf">PDF</a>]</li>
              <li>[<a
              href="fgm.html#code">Code</a>]</li>
              <li>[<a
              href="fgm.html">Project Page</a>]</li>
	         <div id="togContent-fgm" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 This paper integrates techniques in natural language processing
                 and computer vision to improve recognition and description of
                 entities and activities in real-world videos. We propose a
                 strategy for generating textual descriptions of videos by using
                 a factor graph to combine visual detections with language
                 statistics. We use state-of-the-art visual recognition systems
                 to obtain confidences on entities, activities, and scenes
                 present in the video. Our factor graph model combines these
                 detection confidences with probabilistic knowledge mined from
                 text corpora to estimate the most likely subject, verb, object,
                 and place. Results on YouTube videos show that our approach
                 improves both the joint detection of these latent, diverse
                 sentence components and the detection of some individual
                 components when compared to using the vision system alone, as
                 well as over a previous n-gram language-modeling approach. The
                 joint detection allows us to automatically generate more
                 accurate, richer sentential descriptions of videos with a wide
                 array of possible content. 
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap" style="border-bottom: none;">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="imgs/youtube2text.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>YouTube2Text: Recognizing and Describing Arbitrary Activities
           Using Semantic Hierarchies and Zero-shot Recognition
          </br> <small>Sergio Guadarrama,Niveda Krishnamoorthy, Girish
          Malkarnenkar, Subhashini Venugopalan, Raymond Mooney,
          Trevor Darrell, Kate Saenko</small></h3>
          </div>
          <div class="pubv">
            International Conference on Computer Vision, Sydney,
            Australia, December 2013.(ICCV 2013)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-y2t', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Guadarrama_YouTube2Text_Recognizing_and_2013_ICCV_paper.pdf">PDF</a>]</li>
              <li>[<a
              href="https://drive.google.com/file/d/0B7MS0UyZbSS8SDFETEh6WmxVUEE/view?usp=sharing">Poster</a>]</li>
	         <div id="togContent-y2t" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Despite a recent push towards large-scale object recognition,
                 activity recognition remains limited to narrow domains and
                 small vocabularies of actions. In this paper, we tackle the
                 challenge of recognizing and describing activities
                 "in-the-wild". We present a solution that takes a short video
                 clip and outputs a brief sentence that sums up the main
                 activity in the video, such as the actor, the action and its
                 object. Unlike previous work, our approach works on
                 out-of-domain actions: it does not require training videos of
                 the exact activity. If it cannot find an accurate prediction
                 for a pre-trained model, it finds a less specific answer that
                     is also plausible from a pragmatic standpoint. We use
                     semantic hierarchies learned from the data to help to
                     choose an appropriate level of generalization, and priors
                     learned from web-scale natural language corpora to penalize
                     unlikely combinations of actors/actions/objects; we also
                     use a web-scale language model to "fill in" novel verbs,
                     i.e. when the verb does not appear in the training set. We
                     evaluate our method on a large YouTube corpus and
                     demonstrate it is able to generate short sentence
                     descriptions of video clips better than baseline
                     approaches. 
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="showmore" id="showmorepubs">
      <img src="./imgs/double-arrow-down.png" id="arrow"/></div>
      
      <div id="morepubs">
      <hr class="soften"></hr>
      </br>

      <h2>Other Research</h2>
      <p> In the past I have researched on topics in public policy, theoretical cryptography
      and social network analysis.</p>
      </br>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4" style="padding-top: 20px">
          <div class="pubimg">
            <img src="./imgs/etiology_dist.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
            <h3>
              Comparing Supervised Models And Learned Speech Representations For
              Classifying Intelligibility Of Disordered Speech On Selected
              Phrases
          </br> <small>
            Subhashini Venugopalan, Joel Shor, Manoj Plakal, Jimmy Tobin, Katrin
            Tomanek, Jordan Green, Michael Brenner
          </small></h3>
          </div>
          <div class="pubv">
            INTERSPEECH 2021. Brno, Czechia (+Virtual). August 2021.
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-dyscls', this);return
            false">+Abstract</a>] </li>
              <li>[<a
                  href="https://vsubhashini.github.io">PDF</a>]</li>
              <li>[<a
                  href="https://vsubhashini.github.io">Video</a>]</li>
	         <div id="togContent-dyscls" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
Automatic classification of disordered speech can provide an objective tool
for identifying the presence and severity of speech impairment. Classification
approaches can also help identify hard-to-recognize speech samples to teach ASR
systems about the variable manifestations of impaired speech. Here, we develop
and compare different deep learning techniques to classify the intelligibility
of disordered speech on selected phrases. We collected samples from a diverse
set of 661 speakers with a variety of self-reported disorders speaking 29 words
or phrases, which were rated by speech-language pathologists for their overall
intelligibility using a five-point Likert scale. We then evaluated classifiers
developed using 3 approaches: (1) a convolutional neural network (CNN) trained
for the task, (2) classifiers trained on non-semantic speech representations
from CNNs that used an unsupervised objective [1], and (3) classifiers trained
on the acoustic (encoder) embeddings from an ASR system trained on typical
speech [2]. We found that the ASR encoder's embeddings considerably outperform
the other two on detecting and classifying disordered speech. Further analysis
shows that the ASR embeddings cluster speech by the spoken phrase, while the
non-semantic embeddings cluster speech by speaker. Also, longer phrases are
more indicative of intelligibility deficits than single words.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4">
          <div class="pubimg">
            <img src="imgs/solarBOS-VR.jpg"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
           <h3>Topic based classification and pattern identification in patents
          </br> <small>Subhashini Venugopalan, Varun Rai</small></h3>
          </div>
          <div class="pubv">
            Technological Forecasting and Social Change, 2014
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-tfsc', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://www.sciencedirect.com/science/article/pii/S0040162514002923">PDF</a>]</li>
	         <div id="togContent-tfsc" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Patent classification systems and citation networks are used
                 extensively in innovation studies. However, non-unique mapping
                 of classification codes onto specific products/markets and the
                 difficulties in accurately capturing knowledge flows based just
                 on citation linkages present limitations to these conventional
                 patent analysis approaches. We present a natural language
                 processing based hierarchical technique that enables the
                 automatic identification and classification of patent datasets
                 into technology areas and sub-areas. The key novelty of our
                 technique is to use topic modeling to map patents to
                 probability distributions over real world categories/topics.
                 Accuracy and usefulness of our technique are tested on a
                 dataset of 10,201 patents in solar photovoltaics filed in the
                 United States Patent and Trademark Office (USPTO) between 2002
                 and 2013. We show that linguistic features from topic models
                 can be used to effectively identify the main technology area
                 that a patent's invention applies to. Our computational
                 experiments support the view that the topic distribution of a
                 patent offers a reduced-form representation of the knowledge
                 content in a patent. Accordingly, we suggest that this hidden
                 thematic structure in patents can be useful in studies of the
                 policy–innovation–geography nexus. To that end, we also
                 demonstrate an application of our technique for identifying
                 patterns in technological convergence.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap">
      <div class="row">
        <div class="col-md-4">
          <div class="pubimg">
            <img src="imgs/sna-retrieval.png"</img>
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
		  <h3> People and Entity Retrieval in Implicit Social Networks
          </br> <small> Suman K. Pathapati, Subhashini Venugopalan,
          Ashok P. Kumar, Anuradha Bhamidipaty</small></h3>
          </div>
          <div class="pubv">
          IEEE International Conference on Internet Multimedia Systems
          Architecture and Application, Bangalore, India December 2011.
          (IMSAA 2011)
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-basna', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6156373">PDF</a>]</li>
	         <div id="togContent-basna" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
                 Online social networks can be viewed as implicit real
                 world networks, that manage to capture a wealth of
                 information about heterogeneous nodes and edges, which are
                 highly interconnected. Such abundant data can be beneficial in
                 finding and retrieving relevant people and entities within
                 these networks. Effective methods of achieving this
                 can be useful in systems ranging from recommender
                 systems to people and entity discovery systems. Our main
                 contribution in this paper is the proposal of a novel localized
                 algorithm that operates on the sub graph of the social graph
                 and retrieves relevant people or entities. We also demonstrate
                 how such an algorithm can be used in large real world social
                 networks and graphs to efficiently retrieve relevant
                 people/entities.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

      <div class="pwrap" style="border-bottom: none;">
      <div class="row">
        <div class="col-md-4">
          <div class="pubimg">
          </div>
        </div>
        <div class="col-md-8">
          <div class="pubt">
		  <h3> A New Approach to Threshold Attribute Based Signatures
          </br> <small> S. Sharmila Deva Selvi, Subhashini Venugopalan, C
          Pandu Rangan</small></h3>
          </div>
          <div class="publ">
            <ul>
            <li>[<a onclick="toggle('togContent-abs', this);return
            false">+Abstract</a>] </li>
              <li>[<a
              href="https://drive.google.com/file/d/0B7MS0UyZbSS8Q0s0LTM1N2RRNkE/view?usp=sharing">PDF</a>]</li>
	         <div id="togContent-abs" style="display:none">
	           <p class="text-justify">
                 <i>Abstract:</i>
	This work proposes a novel approach to construct threshold attribute based signatures inspired by ring signatures. Threshold
attribute based signatures, defined by a <i>(t, n)</i> threshold predicate, ensure that the signer holds atleast
<i>t</i> out of a specified set of <i>n</i> attributes to pass the verification. Another way to look at this would be
that, the signer has atleast 1 out of the <i>(n \choose t)</i> combination of attribute sets. Thus, a different approach to
t-ABS would be to let the signer pick some <i>n'</i> sets of <i>t</i> attributes each, from the <i>(n \choose t)</i> possible sets, and
prove that (s)he has atleast one of the <i>n'</i> sets in his/her possession. In this work, we provide a flexible
threshold-ABS scheme that realizes this approach and prove it secure with the help of random oracles.
               </p>
	         </div>
            </ul>
          </div>
        </div>
      </div>
      </div>

    </div> <!--morepubs-->

    </div> <!--publications-->
    </div> <!--container-->

    <hr class="soften"></hr>

    <script>
      var more_pubs_shown = false;
      function start() {
      $("#arrow").click(function() {
        if(!more_pubs_shown) {
          $("#morepubs").slideDown('fast', function() {
            $("#arrow").attr('src', './imgs/double-arrow-up.png');
          });
          more_pubs_shown = true;
        } else {
          $("#morepubs").slideUp('fast', function() {
            $("#arrow").attr('src', './imgs/double-arrow-down.png');
          });
          more_pubs_shown = false;
        }
      });
    
      }
    </script>

    <div clas="container">
    <a name="Contact"></a>
    <div class="contact">
      <h1> Contact </h1>

     <span class="glyphicon glyphicon-envelope" aria-hidden="true"></span> <script> obfuscate('utexas.edu','vsubhashini'); </script>
     </br>
     <img src="imgs/GitHub-Mark-32px.png"><a
     href="http://www.github.com/vsubhashini">github.com/vsubhashini</a>
    </div>
    </div> <!--container-->
    

    <footer class="footer">
        <div class="container">
            <p class="text-muted text-center" style="padding-top: 10px">
                <a title="Creative Commons Attribution 4.0 International license" target="_blank" href="http://creativecommons.org/licenses/by/4.0/" rel="license">
                  <img alt="License" src="imgs/cca4-88x31.png"></img>
                </a>
            </p>
        </div>
    </footer>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="./js/bootstrap.min.js"></script>
    <script src="./js/toggle.js"></script>
  </body>
</html>

